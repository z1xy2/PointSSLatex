\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{booktabs}
\usepackage{cleveref}
\crefname{figure}{Fig.}{Figs.}
\crefname{equation}{Eq.}{Eqs.}
\crefname{table}{Table}{Tables}
\renewcommand{\figurename}{Fig.}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}

\begin{document}

\title{PointSS: Geometry-Aware Multi-Scale State Space Feature Learning for Point Clouds}

\author{Xin Wang$^{1,2,3}$, Xinyuan Zhang$^{1}$
\thanks{$^{1}$School of Software, Jilin University, Changchun, China.}
\thanks{$^{2}$School of Computer Science and Technology, Jilin University, Changchun, China.}
\thanks{$^{3}$Key Laboratory of Symbolic Computation and Knowledge Engineering of Ministry of Education, Jilin University, Changchun, China.}
\thanks{Manuscript received TBD; revised TBD.}}

% The paper headers
\markboth{IEEE Transactions on Neural Networks and Learning Systems,~Vol.~XX, No.~X, Month~Year}%
{Wang \MakeLowercase{\textit{et al.}}: PointSS: Geometry-Aware Multi-Scale State Space Feature Learning}

\maketitle

\begin{abstract}
  Recently, State Space Models (SSMs) have exhibited strong global modeling
capabilities and linear computational complexity for point cloud analysis.
However, they often suffer from the loss of spatial proximity and insufficient
hierarchical feature representations due to serializing 3D point clouds into
1D sequences. To address these issues, this paper proposes PointSS, a
geometry-aware multi-scale state space framework. PointSS explicitly injects
geometric priors to alleviate the loss of spatial proximity. Leveraging these
geometric priors, we design an Adaptive Scale-Decoupled State Space Model
(ASD-SSM) featuring a dynamic parameter generation mechanism. This
scale-decoupled approach enables hierarchical feature learning, allowing the
model to simultaneously capture fine-grained local details and long-range
global dependencies. Extensive experiments on the S3DIS and ModelNet40
datasets demonstrate that PointSS achieves state-of-the-art performance in
semantic segmentation and classification tasks while maintaining high computational efficiency.
\end{abstract}

\begin{IEEEkeywords}
Geometry-Aware Feature Learning, Multi-Scale Representation, Point Cloud Serialization,  Point Cloud Analysis, State Space Models
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{P}{oint} clouds are fundamental for 3D perception but pose a challenge to feature extraction due to their sparsity and unstructured nature. Pioneering works such as PointNet and PointNet++ \cite{pointnet,PointNet++} established efficient point-wise processing paradigms. Despite efficiency, their limited ability to capture local structures often leads to sub-optimal prediction accuracy. To address this limitation, Transformers \cite{pt,ptv2,ptv3} subsequently improved semantic representation via self-attention; however, their quadratic complexity $\mathcal{O}(N^2)$ creates scalability bottlenecks.

  Recently, State Space Models (SSMs) \cite{ssm,Mamba}, exemplified by
PointMamba \cite{PointMamba} and PCM \cite{pcm}, have gained attention
due to their linear complexity. However, serializing 3D point clouds into
1D sequences will inevitably sacrifice spatial proximity. As illustrated
in \cref{fig:analysis}, some neighboring points can be far apart
after serialization---points that are spatially close may become
distant in the sequence. Such a loss of
proximity is difficult to mitigate in Mamba's recurrent mechanism,
where the contribution from distant points diminishes before reaching
the current position, preventing effective modeling of local geometric
features such as surface normals and curvature. This issue becomes even
worse at object boundaries (e.g., wall-pillar junctions), where
spatially adjacent points may be far apart in the serialized sequence,
making their context crucial for accurate segmentation.



\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{../picture/analysis.JPG}
	\caption{Visualization of the spatial proximity loss after serialization. Left: Point sequence after serialization. Right: Segmentation prediction results.}
	\label{fig:analysis}
\end{figure}

Existing SSM-based methods struggle to establish effective hierarchical representations due to their reliance on fixed, content-independent parameters. Specifically, single-scale models (e.g., PointMamba\cite{PointMamba}) operate at a uniform spatial resolution, inherently preventing the network from simultaneously capturing fine-grained geometric details and broad semantic context. Conversely, hierarchical approaches (e.g., PCM\cite{pcm})) allocate separate parameters for each downsampled scale. While this captures multi-scale features, these parameters remain spatially invariantâ€”meaning they cannot dynamically adapt to varying local geometric characteristics (e.g., flat surfaces vs. sharp boundaries). Furthermore, allocating independent parameters for every scale introduces prohibitive parameter growth and significant memory overhead. Consequently, the lack of dynamic, geometry-aware parameterization fundamentally limits the capacity of existing SSMs to accurately model complex 3D scenes.


  \begin{enumerate}
	\item We propose a Global Geometry-Aware Mechanism (GGAM) that injects
	explicit geometric priors through windowed neighborhoods and dual-
	serialization fusion, effectively mitigating the loss of spatial proximity
	and providing reliable geometric guidance for scale-adaptive learning.
	
	\item We design an Adaptive Scale-Decoupled State Space Model (ASD-SSM)
	featuring a dynamic parameter generation mechanism. By adapting SSM
	parameters to local geometric characteristics, ASD-SSM enables fine scales
	to capture geometric details while coarse scales model semantic context,
	achieving effective hierarchical representations in a highly parameter-
	efficient manner.
	
	\item Extensive experiments demonstrate that PointSS achieves state-of-the-
	art performance, reaching 75.2\% mIoU on S3DIS and 96.0\% accuracy on
	ModelNet40 while maintaining linear complexity. Comprehensive ablation
	studies further validate the effectiveness of our geometry-aware multi-scale
	design.
\end{enumerate}
Code will be released at https://github.com/z1xy2/PointSS-public.git


\section{Related Work}
\subsection{Deep Learning-based Point Cloud Semantic Analysis}
PointNet \cite{pointnet} pioneered handling the unordered nature of point clouds via symmetric functions. To overcome its limited capacity for local detail modeling, PointNet++ \cite{PointNet++} proposed hierarchical sampling. Subsequent works like DGCNN \cite{DGCNN} and FusionNet \cite{FusionNet} enhanced local feature modeling via dynamic graphs and multi-scale fusion. However, these convolution-based methods remain constrained by limited receptive fields, limiting effective hierarchical feature construction.

Transformers enabled effective global context modeling by leveraging self-attention mechanisms. Point Transformer \cite{pt} integrated positional encoding for spatial awareness, while PTv3 \cite{ptv3} achieved state-of-the-art performance using simplified serialization methods. Despite their advantages, the quadratic complexity $\mathcal{O}(N^2)$ creates bottlenecks for large-scale processing. This forces these models to rely on coarse-grained representations, inherently compromising fine-grained local geometric    
details.

State Space Models (SSMs) \cite{Mamba} offer a promising alternative with linear complexity. Methods like PointMamba \cite{PointMamba} and PCM \cite{pcm} adapt SSMs by serializing 3D clouds into 1D sequences.   However, this adaptation introduces two key challenges. First,
serialization methods disrupt the spatial relationships between
neighboring points critical for local geometric structure, resulting in
spatial proximity loss. Second, uniform serialization struggles to
capture both fine-grained local features and long-range contextual
dependencies, limiting its ability to handle multi-scale representations.


\subsection{Point Cloud Analysis via Geometric Feature Enhancement}

Explicit modeling of geometric attributes, such as surface normals and curvature, is essential for robust 3D structural understanding. PointGA \cite{pointga} injects geometric priors by expanding raw coordinates into multi-dimensional features via triangular positional encoding. Similarly, PointMSGT \cite{pointmsgt} utilizes a multi-scale framework with a Geometric Feature Extraction (GFE) module to reconstruct local triangular structures for detailed relationship modeling. Beyond direct feature extraction, geometric consistency constraints, including distance and angle preservation, have been employed to regularize learning processes \cite{gsrnet,geotransformer}. Geo-CNN \cite{geo-cnn} further captures structural dependencies by constructing geometric graphs with similarity-based edge weights. However, these approaches are often constrained by the high computational overhead of K-Nearest Neighbor (KNN) searches required for precise geometric calculation. Furthermore, effectively integrating explicit geometric priors with deep features while avoiding information redundancy and conflict remains a significant challenge.

\subsection{Point Cloud Analysis via Multi-Scale Feature Fusion}

Multi-scale feature fusion serves as a fundamental mechanism for hierarchical representation in point cloud analysis. Early approaches inspired by Feature Pyramid Networks often suffered from feature redundancy and high computational costs. Recent adaptive strategies have addressed these limitations; Bi et al. \cite{adaptive_fusion_2025} combined sparse and point convolutions via an Importance of Spatial Location module, while Liu et al. \cite{projection_based_fusion_2024} integrated 3D geometry with 2D projections to enhance small object detection. Wen et al. \cite{local_feature_fusion_2024} further utilized multi-layer attention to distinctly encode geometric and semantic features. Despite these advancements, significant challenges persist regarding automatic scale selection, adaptive weight learning, and computational efficiency \cite{lidar_segmentation}. Specifically, existing methods struggle to achieve effective information transmission and dynamic weight allocation across scales in long-sequence scenarios, limiting their capacity for complex scene understanding.

\section{Method}

\subsection{Framework Overview}

The overall architecture of the PointSS model is illustrated in \cref{fig:architecture}. The data processing stage encompasses data augmentation and point cloud serialization, with the latter adopting the encoding scheme from PTv3 \cite{ptv3}. Following data processing, the Global Geometry-Aware Mechanism (GGAM) is employed to perform global geometric feature perception on the point cloud. The feature learning component utilizes a PointNet-based U-shaped encoder-decoder architecture, where both the encoder and decoder implement a point-wise extraction strategy. It should be noted that the Repeated Points (highlighted in orange) refer to an operation performed to ensure uniform window partitioning; specific details are provided in the Window Partitioning section. Subsequently, the point cloud data undergoes final semantic feature fusion, and the ultimate prediction is obtained at the end of the model based on the maximum probability of the label prediction. Compared to PCM, this scheme only requires point cloud serialization at the beginning of training. It performs down-sampling directly via pooling on the serialized point cloud, rather than executing multiple serializations and using K-Nearest Neighbor (KNN) for neighborhood search and down-sampling within the encoder. This approach significantly reduces computational complexity.

\cref{fig:coder} illustrates the basic architecture of the encoder, which includes pooling layers used for downsampling and point cloud information aggregation, along with multiple recurring ASD-SSM modules for deep feature understanding. ASD-SSM is integrated into both the encoder and decoder to achieve high-performance perception of the point cloud. Unlike the encoder, the decoder replaces max pooling with upsampling. Since the decoder also incorporates ASD-SSM, it is capable of learning hierarchical information from different encoder stages during the upsampling process.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{../picture/architecture.jpg}
	\caption{Overall architecture of the PointSS. The encoder and decoder employ a patch-based strategy for point cloud upsampling and down-sampling. The notation (number of points, feature dimension) in Feature Learning indicates the tensor shape at each stage.}
	\label{fig:architecture}
\end{figure}


\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{../picture/coder.JPG}
	\caption{Detailed structure of the encoder module.}
	\label{fig:coder}
\end{figure}


\subsection{Global Geometry-Aware Mechanism (GGAM)}

The difficulty in geometric learning within Mamba-based point cloud analysis methods, as previously discussed, stems fundamentally from the contradiction between the state space model mechanism of Mamba and the geometric characteristics of point clouds. Mamba performs sequence modeling through the following discretized state space equations:
\begin{equation}
x_k = \bar{A}x_{k-1} + \bar{B}u_k, \quad y_k = \bar{C}x_k + \bar{D}u_k
\end{equation}
where, $k$ denotes the position index after point cloud serialization, $x_k$ is the state vector at the $k$-th position, $u_k$ is the input vector, $y_k$ is the output vector, and $\bar{A}$, $\bar{B}$, $\bar{C}$, $\bar{D}$ are the system matrices. During the point cloud serialization process, geometrically adjacent points are separated into ``fracture points" due to serialization rules. Consequently, the state vector $x_k$ struggles to establish associations between these fracture points during linear transmission. This makes it difficult for the model to implicitly learn complete geometric structural features, particularly critical geometric information such as surface normals and curvature that rely on spatial neighborhood definitions.

To address this, the Global Geometry-Aware Mechanism (GGAM) is designed to compensate for the geometric learning difficulties of Mamba by explicitly extracting point cloud geometric priors and establishing global associations, thereby providing rich geometric information for the subsequent encoder and decoder. The overall design of GGAM is illustrated in \cref{fig:ggam}, which primarily consists of two parts: point cloud graph construction with geometric feature extraction, and geometric feature enhancement and fusion.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{../picture/ggam.jpg}
	\caption{Architecture of the Global Geometry-Aware Mechanism (GGAM).}
	\label{fig:ggam}
\end{figure}

Traditional methods for point cloud graph construction and geometric relationship computation (such as curvature and surface normals) typically rely on K-Nearest Neighbor (KNN) searches. However, the computational complexity of KNN searches is prohibitively high, especially when processing large-scale point cloud data. To address this issue, an efficient edge construction and curvature/normal computation method based on point cloud serialization is proposed.

Given the point cloud $\{p_1, p_2, \ldots, p_N\}$ and its serialized sequence $Q = \{q_1, q_2, \ldots, q_N\}$, the sequence is partitioned into non-overlapping windows:
\begin{equation}
W_m = \{q_{(m-1) \cdot P + 1}, q_{(m-1) \cdot P + 2}, \ldots, q_{m \cdot P}\}
\end{equation}
where $m = 1, 2, \ldots, \lfloor N/P \rfloor$ represents the window index, and $P$ denotes the patch size. During the partitioning process, cases may arise where the number of points in the final window is less than $P$. In such instances, a window padding strategy is employed, where points from the penultimate window are padded into the final window. Detailed implementation is provided in the Window Partitioning section.

For any point $p_i$, let its position after serialization be pos($i$). The neighborhood of this point is defined as the window to which it belongs:
\begin{equation}
\mathcal{N}_P(p_i) = W_{\lfloor \frac{\text{pos}(i)-1}{P} \rfloor + 1}
\end{equation}

Based on the windowed neighborhoods, the edge connections of the point cloud are constructed. For points within each window $W_m$, a fully connected graph is established:
\begin{equation}
\mathcal{E}_m = \{(q_i, q_j): q_i, q_j \in W_m, i \neq j\}
\end{equation}

Compared to traditional KNN methods, the windowed neighborhood construction method adopts an efficiency-oriented design philosophy, trading the precision of local neighborhoods for a significant increase in computational efficiency (reducing complexity from O($N$log$N$) to O($N$)). This trade-off strategy is inspired by Point Transformer V3 \cite{ptv3}, which demonstrated that computational resources saved through efficient neighborhood search can be reallocated to expand the model scale (e.g., expanding the receptive field from 16 to 1024 points), ultimately outperforming KNN-based methods across multiple benchmarks.

Based on the constructed windowed neighborhoods, an 8-dimensional feature vector is extracted for each edge $(q_i, q_j)$ to fully describe the geometric relationship between point pairs and construct the point cloud graph. In this graph structure, nodes represent individual points, and edges represent the geometric relationships between pairs of points. The inherent properties of the edge construction in the point cloud graph provide distinct advantages for extracting geometric information. The edge feature vector $\mathbf{f}_{ij}$ consists of the following components:
\begin{equation}
\mathbf{f}_{ij} = [\Delta\mathbf{p}_{ij}, \alpha_{ij}, \boldsymbol{\kappa}] \in \mathbb{R}^8
\end{equation}
where $\Delta\mathbf{p}_{ij} = \mathbf{p}_j - \mathbf{p}_i$ is the relative coordinate vector (3D), and $\alpha_{ij}$ represents the directional relationship feature (3D). The latter is calculated from the difference between the average surface normal within the window and the normalized relative position vector, serving to characterize the deviation of the edge direction relative to the local surface normal. $\boldsymbol{\kappa}$ denotes two types of curvature (2D): the surface variation rate and the Gaussian curvature. After passing through a Multi-Layer Perceptron (MLP), edge features are aggregated into point features by accumulating all incident edge features and dividing by the number of neighboring edges, thereby achieving mean aggregation. For point $p_i$, the aggregated point feature is denoted as $\mathbf{H}$.

The geometric feature enhancement and fusion module consists of three parts: a cross-attention mechanism based on dual-spatial encoding, adaptive gated fusion, and geometric consistency constraints.

Since the aforementioned process is computed in parallel using point clouds serialized via Z-order encoding and Hilbert encoding, each point respectively obtains features $\mathbf{H}_z$ and $\mathbf{H}_h$. These two encoding methods produce different spatial neighborhood relationships and node permutation orders. Due to its recursive binary partitioning characteristic, Z-order encoding exhibits strong spatial clustering at local scales, enabling effective detection of geometric boundaries and local variations. In contrast, the Hilbert curve possesses superior spatial continuity preservation, which better maintains the overall coherence of geometric structures during serialization. Following standard practice, $Q$, $K$, and $V$ are generated within each encoding sequence for self-attention computation, allowing nodes to focus on their neighboring node features under that specific encoding. This yields results $A_z$ and $A_h$ for the Z-order and Hilbert sequences, respectively. Subsequently, a cross-attention mechanism is established between the two encoding sequences to generate $A'_z$ and $A'_h$, ensuring that the feature representations of the same point under two different spatial orderings can mutually enhance each other. Finally, more informative and robust node representations are obtained through feature fusion, the specific process of which is illustrated in \cref{fig:ggam}. The fusion results in the dual-serialization enhanced features $\mathbf{H}''_z$ and $\mathbf{H}''_h$.

After obtaining the enhanced features, a final fusion step is designed. To integrate the two features more intelligently, an adaptive gated unit is developed. It receives the concatenated features and generates a weight for each feature stream (Z-order and Hilbert):
\begin{equation}
[\alpha_z, \alpha_h] = \text{Softmax}(\text{MLP}([\mathbf{H}''_z \| \mathbf{H}''_h]))
\end{equation}
where $\|$ denotes the concatenation operation. The final fused feature $\mathbf{H}_{\text{fused}}$ is obtained via weighted summation:
\begin{equation}
\mathbf{H}_{\text{fused}} = \alpha_z \odot \mathbf{H}''_z + \alpha_h \odot \mathbf{H}''_h
\end{equation}
where $\odot$ denotes the element-wise multiplication.

Since the curvature and surface normals in this method are calculated at the neighborhood granularity, the results are less precise compared to point-wise calculations. Following the introduction of dual-branch learning, a geometric consistency module is incorporated to ensure that the fused features still conform to the geometric laws of the physical world. This module learns a consistency factor $\boldsymbol{\gamma} \in [0, 1]$, which is applied to the fused features:
\begin{align}
\boldsymbol{\gamma} &= \text{Sigmoid}(\text{MLP}([\mathbf{H}''_z \| \mathbf{H}''_h])) \\
\mathbf{H}_{\text{final}} &= \mathbf{H}_{\text{fused}} \odot \boldsymbol{\gamma}
\end{align}

This constraint acts as a soft filter, suppressing geometrically inconsistent feature combinations that may arise during the fusion process. Ultimately, $\mathbf{H}_{\text{final}}$ serves as the output of the GGAM module, incorporating geometry-aware features.


\subsection{Adaptive Scale-Decoupled State Space Model (ASD-SSM)}

\subsubsection{Overall Scheme}
ASD-SSM resolves the modeling constraints caused by parameter sharing by dynamically generating customized SSM parameters for different scales. This method applies a serialized feature extraction scheme to point cloud tasks, constructing a hierarchical point cloud information matrix by parallelly processing multi-layer scale point clouds and fusing information across levels. This enables the model to capture long-range contextual dependencies in long sequences, ensuring high-level semantic understanding while maintaining high-precision processing of local details.

As illustrated in \cref{fig:bss}, the complete processing pipeline of ASD-SSM consists of three core steps: (1) Window Partitioning, where the point cloud is divided into patches according to different scales; (2) Order Prompting, which injects positional information into the serialized point cloud; and (3) Multi-scale Feature Modeling based on adaptive parameter generation, where SSM parameters are dynamically modulated for different scales via a parameter generator. The figure demonstrates the feature processing flow for $S=3$ scales, with dashed lines separating the patches. The first layer (finest scale) uses a base window size $P$ for serialization. The window expansion factors $F_s$ for the second and third layers are 2 and 4, respectively, resulting in patch sizes of $2P$ and $4P$. Within each layer, an adaptive parameter-generated Mamba is employed for feature extraction, including forward SSM, backward SSM, and residual connections. Upon completion of feature extraction across scales, a concatenation strategy is used to fuse the multi-layer features point-wise. Finally, feature mapping is completed through an MLP and a linear layer to obtain the final point cloud representation Points$' \in \mathbb{R}^{N \times C}$, which incorporates multi-level information.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{../picture/BSS.JPG}
	\caption{Detailed pipeline of the Adaptive Scale-Decoupled State Space Model (ASD-SSM).}
	\label{fig:bss}
\end{figure}

Notation: In the following descriptions, $s \in \{1, 2, \ldots, S\}$ denotes the scale index of the multi-scale layers, where $s=1$ represents the finest scale (corresponding to the first layer with a window size $P$), and $s=S$ represents the coarsest scale (with a window size $P \times F_s$). $N$ denotes the number of points, and $C$ represents the feature dimension of the point cloud.

\subsubsection{Window Partitioning}

Window partitioning is the initial step of ASD-SSM, aimed at dividing the continuous point cloud sequence into multiple fixed-size patches to establish a foundation for subsequent multi-scale feature extraction. This module primarily consists of two sub-processes: Padding and Patch Grouping.

Padding: To ensure that the point cloud can be uniformly partitioned, the sequence is first padded to an integer multiple of the patch size. Given the total number of points $N$ and the patch size $P$, if $N$ is not divisible by $P$, the sequence is padded by replicating the last few points.

Patch Grouping: After padding, the point cloud sequence is grouped uniformly. For the base layer ($s=1$), the patch size is $P$ with an expansion factor $F_1 = 1$. The partitioned point cloud is represented as Points$_1 \in \mathbb{R}^{\lceil N/P \rceil \times P \times C}$. For higher-scale layers where $s > 1$, the patch size is expanded to $P \times F_s$, and the partitioned point cloud for that layer is represented as Points$_s \in \mathbb{R}^{\lceil N/(P \times F_s) \rceil \times (P \times F_s) \times C}$.

Boundary Handling: In practice, the final patch may contain an insufficient number of points. To maintain local continuity, a sliding window strategy is adopted: the trailing points of the penultimate patch are used to pad the beginning of the final patch. Although this introduces a small number of redundant points, it ensures that the points within the final patch maintain spatial proximity, thereby avoiding the introduction of unrelated padding points.

\subsubsection{Order Prompt}

Since ASD-SSM adopts multiple serialization strategies (e.g., Z-order and Hilbert curves), an Order Prompting mechanism is introduced to help the model understand and utilize different point cloud arrangement rules. This mechanism informs the model of the arrangement rules followed by the current sequence through learnable vector flags, enabling the model to learn order-dependent patterns.

Prompt Vector Design: A set of unique learnable vectors $\{v_1, v_2, \ldots, v_L\}$ is created for each serialization method, where $L$ denotes the number of prompt vectors (set to 6--8 in experiments). Before use, these vectors are mapped to the same dimension $C$ as the current point cloud features through a linear layer. These prompt vectors are then inserted at the beginning and end of each patch to form an augmented sequence.

Hierarchical Strategy: At the finest scale layer ($s =1$), since the point cloud is arranged in serialized order, order prompts are added to reinforce positional information. In higher-scale layers ($s>1$), the point cloud within each patch of Points$_s$ is shuffled to encourage the SSM to learn features while respecting the inherent permutation invariance (disorder) of point clouds. In these cases, since the order has been shuffled, order prompts are no longer required.

This design draws inspiration from the approach used in Vision Mamba \cite{VisionMamba} for processing block-based data, providing the model with order information at the start of the sequence to guide it in adjusting its feature extraction strategy accordingly.

\subsubsection{Multi-scale Feature Modeling Based on Adaptive Scale-aware Parameter Generation}

Traditional SSM methods employ identical state transition parameters $\bar{A}$, input projection parameters $\bar{B}$, and output projection parameters $\bar{C}$ to process features across all scales, overlooking the differentiated requirements of temporal modeling for different scales. The state transition parameter $\bar{A}$ (diagonal elements of the diagonal matrix) controls the decay rate of the state vector: when the elements of $\bar{A}$ are close to 1, slow decay (long-term memory) is achieved; when they are close to 0, rapid decay (fast response) occurs. For coarse-scale features, $\bar{A}$ needs to be close to an all-ones vector to maintain long-range dependencies; for fine-scale features, smaller values of $\bar{A}$ are required to respond quickly to local detailed variations.

Scale-aware Parameter Generator: ASD-SSM adaptively generates differentiated state transition parameters $\bar{A}_s$ for different scales. For a given scale $s$, the point cloud features have been partitioned into patches as $\mathbf{P}_s \in \mathbb{R}^{M \times P \times C}$, where $M = \lceil N/P \rceil$ is the number of patches. The parameter generation process is conducted independently for each patch, creating an exclusive $\bar{A}_s$ based on the content features of each patch.

Specifically, we first perform global adaptive average pooling on the $P$ points within each patch to obtain the global feature of that patch, $\mathbf{f}_{\text{global}}^{(s)} \in \mathbb{R}^{C/4}$. This feature captures the overall geometric characteristics within the patch (e.g., local structural complexity, density distribution, etc.). Simultaneously, we learn a scale embedding vector $\mathbf{e}_s \in \mathbb{R}^{C/4}$ for each scale to distinguish the characteristics across different scales. For a single patch, the parameter generation process is as follows:
\begin{align}
\mathbf{f}_{\text{global}}^{(s)} &= \text{MLP}_{\text{global}}(\mathbf{f}_{\text{global}}^{(s)}) \in \mathbb{R}^{C/4} \\
\mathbf{c}_s &= \text{MLP}([\mathbf{f}_{\text{global}}^{(s)} \| \mathbf{e}_s]) \in \mathbb{R}^{C/2}
\end{align}
where $\|$ denotes the concatenation operation. For the selective parameters $\bar{B}_s$ and $\bar{C}_s$, we follow the standard selective mechanism from the Mamba paper \cite{Mamba}, dynamically generating them from the input content via linear projections.

Scale Constraint Mechanism: To achieve differentiated state decay characteristics, the value range of the state transition matrix $\bar{A}_s$ is explicitly controlled via the scale constraint factor $\alpha_s$. Within the framework of discrete state space models, the state update rule is governed by $\mathbf{h}_k = \bar{A}_s\mathbf{h}_{k-1} + \bar{B}_s\mathbf{u}_k$. where, the diagonal elements of $\bar{A}_s$ dictate the state decay rate; specifically, elements approaching 1 facilitate slow decay (capturing long-range memory), whereas elements approaching 0 induce fast decay (enabling rapid response).

To directly constrain the range of eigenvalues of $\bar{A}_s$, the output of the parameter generator is first mapped to the interval [0,1] via a sigmoid function:
\begin{equation}
\Delta\bar{A}_s^{\text{norm}} = \sigma(\lambda \cdot \text{MLP}([\mathbf{f}_{\text{global}}^{(s)} \| \mathbf{e}_s]))
\end{equation}
where $\sigma(\cdot)$ is the sigmoid activation function, and $\lambda$ is the modulation intensity coefficient (set to 0.1 in experiments), used to control the amplitude of the MLP output. Then, the scale constraint factor $\alpha_s$ serves as an upper bound coefficient to directly control the value range of $\bar{A}_s$:
\begin{equation}
\bar{A}_s = \alpha_s \cdot \Delta\bar{A}_s^{\text{norm}}
\end{equation}

By setting $\alpha_s$ to increase from fine to coarse scales ($\alpha_1 = 0.3 \rightarrow \alpha_S = 0.9$), we impose an explicit constraint: the fine-scale $\bar{A}_1 \in [0, 0.3]$ enables a rapid response to local variations, while the coarse-scale $\bar{A}_S \in [0, 0.9]$ maintains long-term memory to capture global context. It should be emphasized that, although the patch index is omitted in the notation for simplicity, each patch generates its exclusive parameter $\bar{A}_s^{(m)}$ based on its specific content features, where $m$ denotes the patch index. Specifically, $\bar{A}_s^{(m)} = \alpha_s \cdot \Delta\bar{A}_s^{\text{norm},(m)}$, where $\Delta\bar{A}_s^{\text{norm},(m)} = \sigma(\lambda \cdot \text{MLP}([\mathbf{f}_{\text{global}}^{(s,m)} \| \mathbf{e}_s]))$.

State Space Update: For the point cloud feature sequence within each patch at the $s$-th scale, the state space update follows the standard discretized SSM formulation \cite{Mamba}:
\begin{align}
\mathbf{h}_k^{(s,m)} &= \bar{A}_s\mathbf{h}_{k-1}^{(s,m)} + \bar{B}_s\mathbf{u}_k^{(s,m)} \\
\mathbf{y}_k^{(s,m)} &= \bar{C}_s\mathbf{h}_k^{(s,m)} + \bar{D}\mathbf{u}_k^{(s,m)}
\end{align}
where $\mathbf{h}_k^{(s,m)}$ is the hidden state vector for scale $s$ and patch $m$, $\mathbf{u}_k^{(s,m)}$ is the input vector of the $k$-th point within the patch ($k = 1,2, \ldots, P$), and $\bar{D}$ denotes the skip connection parameter. For the $m$-th patch, the initial state is set as $\mathbf{h}_0^{(s,m)} = \mathbf{h}_P^{(s,m-1)}$ (initialized as a zero vector for the first patch), and the aforementioned update formulas are applied starting from $k = 1$. This direct transmission design maintains the consistency of state transitions in the SSM, ensuring that state updates at patch boundaries remain coherent with those within the patches.

Bidirectional Feature Learning and Fusion: A bidirectional feature learning strategy is employed within each scale. To enable each point to capture both preceding and succeeding contextual information, we construct a forward SSM and a backward SSM for each scale $s$, processing the sequence in forward and reverse orders, respectively. The forward and backward SSMs share the same set of scale-specific state transition parameters $\bar{A}_s$. Finally, the bidirectional features are integrated via residual connections.

Through the aforementioned mechanisms, ASD-SSM achieves scale-decoupled state-space modeling: at the finest scale ($s = 1$), the constraint $\alpha_1 = 0.3$ ensures $\bar{A}_1 \in [0,0.3]$, allowing for rapid state decay to respond to local detailed variations; at the coarsest scale ($s = S$), the constraint $\alpha_S = 0.9$ ensures $\bar{A}_S \in [0,0.9]$, enabling slow state decay to maintain long-range memory for capturing global semantics. Upon completion of feature extraction across multiple scales, the features Points$_{1:S}$ from different scales are fused point-wise using a concatenation strategy. Finally, the fused representation is mapped back to the original dimension $C$ through an MLP and a linear layer, resulting in the multi-level informed point cloud representation Points$' \in \mathbb{R}^{N \times C}$.

\section{Experiments}

Implementation Details: We conducted benchmark tests for point cloud analysis on the indoor dataset S3DIS and the classification dataset ModelNet40. All experiments were performed on an Nvidia A6000 GPU. For network training, we employed the AdamW optimizer and the OneCycleLR learning rate scheduler, with an initial maximum learning rate of 6e-3. The scheduling mechanism included an early-stage warm-up followed by a cosine annealing cycle. The entire training process spanned 3000 epochs with a batch size of 6, utilizing Automatic Mixed Precision (AMP) to enhance training efficiency.

In the encoder, the channel dimensions were set to (32, 64, 128, 256, 512), while the decoder channel dimensions were set to (64, 64, 128, 256). The Patch Size was uniformly set to 128, consistent with existing works to ensure a fair comparison. We selected the Cross-Entropy loss as the evaluation criterion, which is the most widely used loss function for classification and segmentation tasks in semantic analysis.

To fairly evaluate the independent contributions of our proposed architectural innovations (GGAM and ASD-SSM), we adopted a train-from-scratch setting, consistent with current Mamba-based methods. This choice ensures that performance gains are solely attributable to architectural improvements, eliminating confounding factors arising from diverse pre-training strategies.

\subsection{Datasets}

S3DIS: The S3DIS dataset serves as a standard benchmark for indoor semantic segmentation. It contains 272 rooms across six distinct areas, covering 13 semantic categories (e.g., walls, furniture). Each point includes 3D coordinates and RGB information. Following established protocols, Area 5 is selected for result validation.

ModelNet40: This dataset is utilized for 3D object classification, comprising 12,311 CAD models from 40 categories. The data is partitioned into 8,156 training samples and 4,155 testing samples.

\subsection{Evaluation Metrics}

mIoU: Mean Intersection over Union (mIoU) is a widely used evaluation metric in tasks such as semantic segmentation to measure the degree of overlap between predicted results and ground truth labels. For each category $c$, the Intersection over Union (IoU) is defined as:
\begin{equation}
\text{IoU}_c = \frac{\text{TP}_c}{\text{TP}_c + \text{FP}_c + \text{FN}_c}
\end{equation}
where TP$_c$ denotes the number of points correctly predicted as category $c$ (True Positives), FP$_c$ represents the number of points incorrectly predicted as category $c$ (False Positives), and FN$_c$ represents the number of points that truly belong to category $c$ but were not correctly predicted (False Negatives). The mIoU is obtained by taking the arithmetic mean of the IoU values across all semantic categories. Since mIoU provides a more comprehensive reflection of the model's segmentation performance across various classes---especially in cases of imbalanced class distributions---it is more robust than Overall Accuracy. Therefore, this paper adopts mIoU as the primary metric for evaluating point cloud semantic segmentation performance on the S3DIS dataset.

OA: Overall Accuracy (OA) is defined as the ratio of the number of correctly predicted samples to the total number of samples across all point cloud data. It is one of the most intuitive and commonly used performance metrics in classification tasks. This metric effectively reflects the model's discriminative capability at the global classification level. Consequently, this paper utilizes OA as the evaluation metric for point cloud classification performance on the ModelNet40 dataset.

\subsection{Quantitative Experiments}

\subsubsection{Experiment Results on the S3DIS Dataset}

\cref{tab:s3dis} summarizes the semantic segmentation performance on the S3DIS dataset. Without pre-training, PointSS achieves 75.2\% mIoU in Area 5, significantly outperforming mainstream methods. Specifically, it surpasses the state-of-the-art Point Transformer V3 by 2.0\% and the representative state-space method PCM by 5.4\%.

These improvements are primarily attributed to two architectural innovations. First, the Global Geometry-Aware Mechanism (GGAM) mitigates serialization-induced information loss by integrating windowed neighborhoods and dual-serialization fusion. This enhances boundary continuity and semantic consistency, particularly in complex regions with sharp transitions. Second, the Adaptive Scale-Decoupled State Space Model (ASD-SSM) dynamically generates customized parameters across scales. This decoupling enables the simultaneous modeling of long-range global dependencies at coarse scales and rapid local responses at fine scales, overcoming the limitations of single-granularity shared parameters.

Furthermore, PointSS retains the linear time complexity of SSMs. As shown in \cref{tab:param_comparison}, the method maintains high computational efficiency with only a marginal increase in inference time, validating its scalability for large-scale point cloud scenarios.

\begin{table}[!t]
\centering
\caption{Performance Comparison of Semantic Segmentation on S3DIS}
\label{tab:s3dis}
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Source} & \textbf{Model} & \textbf{Architecture} & \textbf{mIoU} \\ \midrule
NeurIPS22 & Point Transformer V2\cite{ptv2} & Transformer & 72.6 \\
CVPR23 & PointMetaBase\cite{pmb} & Transformer & 72.0 \\
ICCV23 & SuperpointTransformer\cite{spt} & Transformer & 68.1 \\
CVMJ23 & Swin3D\cite{Swin3D} & Transformer & 72.5 \\
CVPR24 & PPT+SpareUNET\cite{ppt} & Transformer & 72.7 \\
CVPR24 & OA-CNNs\cite{oacnn} & CNN & 71.1 \\
CVPR24 & OneFormer3D\cite{OneFormer3D} & Transformer & 72.4 \\
CVPR24 & Point Transformer V3\cite{ptv3} & Transformer & 73.2 \\
AAAI25 & PCM\cite{pcm} & SSM & 69.8 \\
Mathematics24 & PointMSGT\cite{pointmsgt} & Transformer & 68.6 \\
Sci. Rep.25 & PointGA\cite{pointga} & Transformer & 66.2 \\
& PointSS & SSM & \textbf{75.2} \\ \bottomrule
\end{tabular}
\end{table}

\subsubsection{Experiment Results on the ModelNet40 Dataset}

\cref{tab:ModelNet} summarizes the classification performance on ModelNet40. PointSS achieves an Overall Accuracy (OA) of 96.0\% without pre-training, outperforming the state-of-the-art Transformer-based Point2Vec by 1.2\% and the SSM-based Mamba3D by 1.9\%. This establishes PointSS as the top-performing SSM approach. The superior results compared to masked self-supervised methods indicate that explicit structural modeling tailored to point clouds can effectively enhance global semantic understanding without relying on large-scale pre-training. By preserving local geometric details through multi-scale hierarchical aggregation, the model demonstrates robust discriminative capability in classification tasks.

\begin{table}[!t]
\centering
\caption{Classification Performance Comparison on ModelNet40}
\label{tab:ModelNet}
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Source} & \textbf{Model} & \textbf{Architecture} & \textbf{OA} \\ \midrule
NeurIPS22 & Point Transformer V2 \cite{ptv2} & Transformer & 94.2 \\
AAAI24 & Point-FEMAE \cite{FEMAE} & Transformer & 94.5 \\
LNCS24 & Point2Vec \cite{Point2Vec} & Transformer & 94.8 \\
ACM MM24 & Mamba3D \cite{Mamba3D} & SSM & 94.1 \\
NIPS24 & PointMamba \cite{PointMamba} & SSM & 93.6 \\
TIP25 & OTMae3D \cite{OTMae3D} & Transformer & 94.5 \\
AAAI25 & PCM \cite{pcm} & SSM & 93.4 \\
Sci. Rep.25 & PointGA \cite{pointga} & Transformer & 93.8 \\
& PointSS & SSM & \textbf{96.0} \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Ablation Study}

Ablation studies were conducted on the S3DIS dataset using a progressive strategy to verify individual module contributions. First, the effectiveness of GGAM in addressing geometric loss was examined (\Cref{tab:ggam_fragmentation,tab:ggam_category,tab:ggam_components}), followed by the validation of ASD-SSM design choices (\Cref{tab:scale_constraint,tab:param_gen,tab:param_comparison,tab:scale_config}).

The baseline model utilizes a standard single-scale bidirectional Mamba with shared parameters. As shown in \Cref{tab:progressive_improvement}, introducing GGAM alone yields a 2.2\% mIoU improvement. This confirms that extracting geometric priors through windowed neighborhoods and dual-serialization effectively mitigates serialization-induced information loss, thereby enhancing local structural representation.

Building on this, incorporating multi-scale modeling with shared parameters improves performance by 0.9\% to 73.4\% mIoU. However, employing the full ASD-SSM with scale-aware parameter generation boosts performance to 75.2\%, providing an additional 1.8\% gain over the GGAM baseline. This demonstrates that decoupling parameters is crucial for simultaneously optimizing local detail capture and global semantic modeling.

In summary, GGAM and ASD-SSM exhibit strong functional complementarity: GGAM restores geometric integrity, while ASD-SSM balances multi-scale dependencies. Their synergy achieves a total performance improvement of 4.9\% mIoU while maintaining high computational efficiency.

\begin{table*}[!t]
\centering
\caption{INCREMENTAL PERFORMANCE IMPROVEMENT PATH ON THE S3DIS DATASET}
\label{tab:progressive_improvement}
\begin{tabular}{lll}
\toprule
\textbf{Model Configuration} & \textbf{mIoU (\%)} & \textbf{Improvement} \\
\midrule
Pure Mamba Baseline & 70.3 & - \\
Baseline + GGAM & 72.5 & +2.2 \\
Baseline + GGAM + Shared-Parameter ASD-SSM & 73.4 & +0.9 \\
Baseline + GGAM + Full ASD-SSM (PointSS) & \textbf{75.2} & +1.8 \\
\midrule
\textbf{Total Improvement} & - & \textbf{+4.9} \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[!t]
	\centering
	\caption{Point Distribution Across Different Fragmentation Levels and the Compensation Effect of GGAM}
	\label{tab:ggam_fragmentation}
	\begin{tabular}{llllll}
		\toprule
		\textbf{Fragmentation} & \textbf{Point} & \textbf{Point} & \textbf{Baseline} & \textbf{GGAM} & \textbf{Absolute} \\
		\textbf{Ratio} & \textbf{Count} & \textbf{Percentage} & \textbf{IoU} & \textbf{IoU} & \textbf{Gain} \\
		\midrule
		0-10\% (Low) & 270k & 67\% & 74.0 & 74.8 & 0.8 \\
		10-20\% (Mild) & 101k & 25\% & 63.0 & 67.1 & 4.1 \\
		20-30\% (Moderate) & 32k & 8\% & 62.0 & 70.0 & 8.0 \\
		\midrule
		Total & 403k & 100\% & 70.3 & 72.5 & 2.2 \\
		\bottomrule
	\end{tabular}
	\par\vspace{3pt}
	\footnotesize Note: The fragmentation ratio is defined as the number of points among a point's K-nearest neighbors whose sequence distance exceeds 500, divided by K.
\end{table*}
\subsubsection{Ablation Study of GGAM}

Systematic ablation experiments were conducted to verify the effectiveness of the Global Geometry-Aware Mechanism (GGAM) in mitigating the difficulties of geometric learning introduced by point cloud serialization. All experiments were performed on the S3DIS dataset, with a focus on analyzing the neighborhood fragmentation phenomenon caused by serialization, the compensation capability of GGAM across categories of varying geometric complexity, and the synergistic mechanisms of the internal components within GGAM.

To quantitatively characterize the neighborhood fragmentation issue caused by serialization, the distribution of the 128 spatial neighbors for each point within the sequence was analyzed under Hilbert curve serialization. Specifically, for each point, the proportion of its neighbors with a sequence distance greater than 500 was calculated. Based on this metric, points were categorized into different fragmentation intervals. Subsequently, the segmentation performance of the baseline model versus the model with GGAM was statistically evaluated across these various fragmentation levels, as presented in \Cref{tab:ggam_fragmentation}.

Experimental results indicate that approximately 33\% of points undergo neighborhood fragmentation under Hilbert serialization. As the fragmentation ratio increases from 0--10\% to 20--30\%, baseline performance drops sharply from 74.0 to 62.0, confirming the detrimental impact of serialization on local geometric representation. GGAM effectively mitigates this degradation, exhibiting a positive correlation between fragmentation levels and performance gains. Notably, an absolute gain of 8.0 is observed in the high-fragmentation interval (20--30\%), significantly exceeding improvements in low-fragmentation areas.



This trend is further corroborated by category-wise analysis (\Cref{tab:ggam_category}). Large planar structures with high continuity (e.g., walls, floors) show limited gains (1.0--1.4). In contrast, categories featuring complex boundaries and slender structures (e.g., columns, clutter) exhibit higher fragmentation ratios and achieve substantial improvements (5.3--6.5). These findings demonstrate that GGAM is particularly effective at recovering critical geometric information in regions with sharp transitions and dense boundaries.

\begin{table*}[!t]
\centering
\caption{Category-wise Performance Analysis of GGAM on the S3DIS Dataset}
\label{tab:ggam_category}
\begin{tabular}{llllll}
\toprule
\textbf{Category} & \textbf{Avg. Fragmentation Ratio} & \textbf{Baseline IoU} & \textbf{GGAM IoU} & \textbf{Absolute Gain} & \textbf{Geometric Characteristics} \\
\midrule
Floor & 5\% & 92.3 & 93.5 & +1.2 & Large planar surface \\
Wall & 6\% & 88.7 & 90.1 & +1.4 & Large planar surface \\
Ceiling & 8\% & 90.5 & 91.9 & +1.4 & Large planar surface \\
Table & 11\% & 73.5 & 76.6 & +3.1 & Rectangular surface with legs \\
Bookshelf & 13\% & 70.2 & 73.9 & +3.7 & Vertical structure \\
Chair & 15\% & 68.9 & 73.2 & +4.3 & Complex structure \\
Sofa & 15\% & 70.6 & 74.8 & +4.2 & Curved surface with backrest \\
Door & 17\% & 65.2 & 70.5 & +5.3 & Rectangular boundaries \\
Column & 18\% & 54.3 & 60.1 & +5.8 & Slender vertical structure \\
Window & 19\% & 61.8 & 67.3 & +5.5 & Complex boundaries \\
Clutter & 22\% & 45.7 & 52.2 & +6.5 & Highly complex \\
\midrule
Mean & 13.5\% & 70.3 & 72.5 & +2.2 & - \\
\bottomrule
\end{tabular}
\par\vspace{3pt}
\footnotesize Note: The Avg. Fragmentation Ratio is defined as the mean fragmentation ratio across all points belonging to a specific semantic category.
\end{table*}

To further verify the independent contributions and synergistic relationships among the constituent components of GGAM, a progressive ablation study was conducted on its core elements, with the results summarized in \Cref{tab:ggam_components}.

As indicated by the experimental results, the introduction of geometric features alone leads to a performance gain of 0.8\% mIoU, suggesting that explicit geometric priors provide a more reliable foundation for modeling local structural information. Conversely, the individual implementation of the dual-serialization strategy yields limited performance improvements or even slight fluctuations. This observation implies that different serialization methods may introduce directional biases or neighborhood perturbations, which fail to produce stable gains without an effective interaction mechanism.

Furthermore, when geometric features are integrated with dual-serialization, the performance improves to 71.9\% mIoU. This indicates that multiple serialization sequences can provide complementary neighborhood information to a certain extent.

Building upon this, the introduction of the cross-sequence attention mechanism increases the model performance to 72.2\% mIoU, demonstrating that cross-sequence interaction plays a pivotal role in aligning complementary geometric information across different serialization patterns. The subsequent addition of the gated fusion mechanism and geometric consistency constraints further elevates the performance to 72.5\% mIoU. The gated mechanism adaptively balances the contributions of different serialization branches based on feature quality, while the geometric consistency constraints suppress unreasonable geometric combinations caused by window-approximated neighborhoods. This progressively enhances the stability and robustness of the feature fusion process.

In summary, the performance gain of GGAM is not driven by a single component but is the result of the synergistic effects of multiple mechanisms. Through geometric prior injection, dual-serialization complementarity, cross-sequence feature interaction, and adaptive fusion constraints, GGAM effectively mitigates the geometric information loss induced by point cloud serialization. Ultimately, an overall improvement of 2.2\% mIoU is achieved on the S3DIS dataset, validating the rationality and effectiveness of the proposed design.

\begin{table}[!t]
\centering
\caption{Component-wise Ablation Study of the GGAM on the S3DIS Dataset}
\label{tab:ggam_components}
\begin{tabular}{cccccc}
\toprule
\textbf{GF} & \textbf{DS} & \textbf{CSA} & \textbf{GFM} & \textbf{$L_{geo}$} & \textbf{mIoU (\%)} \\
\midrule
$\times$ & $\times$ & $\times$ & $\times$ & $\times$ & 70.3 \\
\checkmark & $\times$ & $\times$ & $\times$ & $\times$ & 71.1 \\
\checkmark & \checkmark & $\times$ & $\times$ & $\times$ & 71.0 \\
\checkmark & \checkmark & \checkmark & $\times$ & $\times$ & 71.9 \\
\checkmark & \checkmark & \checkmark & \checkmark & $\times$ & 72.2 \\
\checkmark & \checkmark & \checkmark & \checkmark & \checkmark & 72.5 \\
\bottomrule
\end{tabular}
\par\vspace{3pt}
\footnotesize Notes: GF: Geometric Features; DS: Dual-Serialization; CSA: Cross-Sequence Attention; GFM: Gated Fusion Mechanism; $L_{geo}$: Geometric Consistency Constraint (Loss).
\end{table}

\subsubsection{ASD-SSM Ablation Study}

Systematic ablation experiments on the S3DIS dataset evaluated key ASD-SSM design choices, focusing on the scale constraint factor, parameter generator, and scale configuration. First, the scale constraint factor was analyzed for its role in regulating the state transition matrix. Smaller factors induce rapid state decay, facilitating the capture of local geometric variations, whereas larger factors delay decay, enhancing the modeling of long-range dependencies and global semantics (\Cref{tab:scale_constraint}).

Experimental results indicate that uniform scale constraint factors consistently underperform compared to differentiated configurations, confirming that distinct scales require specific state-decay characteristics. Optimal performance is achieved when constraint factors increase progressively from fine to coarse scales (0.3, 0.6, 0.9). This validates the design philosophy: fine scales benefit from rapid decay to capture local variations, while coarse scales require slow decay to maintain long-range memory. Reversing this configuration drops performance to 72.9\% mIoU, as excessive memory at fine scales interferes with local modeling, and rapid decay at coarse scales disrupts global consistency.

\begin{table}[!t]
\centering
\caption{Ablation Study of Scale Constraint Factors $\alpha_s$ with a 3-Scale Configuration}
\label{tab:scale_constraint}
\begin{tabular}{cccc}
\toprule
$\alpha_1$ & $\alpha_2$ & $\alpha_3$ & \textbf{mIoU} \\
\midrule
0.3 & 0.3 & 0.3 & 73.8 \\
0.7 & 0.7 & 0.7 & 74.1 \\
0.5 & 0.5 & 0.5 & 74.3 \\
0.3 & 0.5 & 0.7 & 75.0 \\
0.3 & 0.6 & 0.9 & 75.2 \\
0.3 & 0.7 & 0.9 & 75.2 \\
0.2 & 0.6 & 0.9 & 75.1 \\
0.4 & 0.6 & 0.9 & 75.0 \\
0.9 & 0.6 & 0.3 & 72.9 \\
\bottomrule
\end{tabular}
\end{table}

Based on these findings, the parameter generator was analyzed (\Cref{tab:param_gen}). By leveraging both global features and scale information, the generator produces scale-dependent parameters that further enhance performance. Comparative schemes demonstrate that integrating both information sources outperforms using either in isolation.

Experimental results indicate that both global features and scale embeddings independently enhance model performance, with global features providing a slightly higher improvement than scale information alone. This suggests that scene-level global semantics provide a critical reference for state parameter generation, enabling the model to adaptively adjust state-updating behaviors across different regions. When global features and scale information are integrated concurrently, the performance gain reaches 1.8\% mIoU, which is substantially higher than the improvement from either source alone, demonstrating a clear synergy in the parameter generation process. Further analysis of different modulation strengths ($\lambda$) reveals that optimal performance is achieved at $\lambda$ = 0.1. Insufficient modulation strength limits the model's adaptive capacity, while excessive strength may introduce unstable perturbations, compromising the smoothness of the state-updating process.

\begin{table*}[!t]
\centering
\caption{Ablation Study on the Design of the Parameter Generator}
\label{tab:param_gen}
\begin{tabular}{llllll}
\toprule
\textbf{Scheme} & \textbf{Global Features} & \textbf{Scale Embedding} & \textbf{Modulation Strength ($\lambda$)} & \textbf{mIoU (\%)} & \textbf{Gain ($\Delta$)} \\
\midrule
Baseline (Shared) & - & - & - & 73.4 & - \\
Scheme 1 & $\times$ & \checkmark & 0.1 & 74.1 & +0.7 \\
Scheme 2 & \checkmark & $\times$ & 0.1 & 74.3 & +0.9 \\
Scheme 3 & \checkmark & \checkmark & 0.05 & 74.8 & +1.4 \\
Scheme 4 & \checkmark & \checkmark & 0.1 & 75.2 & +1.8 \\
Scheme 5 & \checkmark & \checkmark & 0.2 & 74.6 & +1.2 \\
Scheme 6 & \checkmark & \checkmark & 0.3 & 73.9 & +0.5 \\
\bottomrule
\end{tabular}
\end{table*}

Subsequently, ASD-SSM was compared with several common parameterization strategies, as summarized in \Cref{tab:param_comparison}. The Shared Parameter scheme is the most lightweight in terms of parameter count and computational efficiency; however, it fails to capture functional differences across scales, leading to relatively low overall performance. The Independent Parameters scheme achieves the highest segmentation performance but at a significant cost in terms of parameter volume and memory footprint. The Parameter Interpolation approach controls the parameter count through linear generation between scales; yet, linear interpolation struggles to represent complex non-linear relationships across scales, resulting in limited gains. While Low-Rank Adaptation (LoRA) is efficient in parameter count, its static low-rank constraints limit the model's representational capacity. In contrast, by employing a dynamic parameter generation mechanism, ASD-SSM achieves performance comparable to the independent parameter scheme with only an approximately 20\% increase in parameter volume, demonstrating superior parameter efficiency. Notably, the differences in inference time among these methods are negligible, indicating that ASD-SSM significantly improves performance without introducing substantial computational overhead.

\begin{table*}[!t]
\centering
\caption{Performance and Efficiency Comparison of Different Parameterization Methods}
\label{tab:param_comparison}
\begin{tabular}{lllll}
\toprule
\textbf{Parameterization Method} & \textbf{Params} & \textbf{Inf. Time} & \textbf{GPU Memory} & \textbf{mIoU (\%)} \\
\midrule
Shared Parameters & 1.0$\times$ & 11.2 ms & 45.5 GB & 73.4 \\
Independent Parameters & 3.0$\times$ & 12.1 ms & 51.3 GB & 75.8 \\
Parameter Interpolation & 2.0$\times$ & 11.5 ms & 46.8 GB & 74.2 \\
LoRA Adaptation & 1.15$\times$ & 11.8 ms & 47.2 GB & 74.6 \\
ASD-SSM (Ours) & 1.2$\times$ & 11.9 ms & 47.8 GB & 75.2 \\
\bottomrule
\end{tabular}
\end{table*}

A systematic investigation into scale quantities and window expansion configurations (\Cref{tab:scale_config}) confirms the necessity of hierarchical modeling. Introducing a second scale significantly outperforms the single-scale baseline, while incorporating a third scale yields a further 1.6\%--1.9\% improvement. This suggests that a three-level hierarchy---local, intermediate, and global---provides the most comprehensive structural representation.

Regarding window expansion, moderate increases enhance context modeling, whereas excessive sizes tend to dilute spatial features. Although four- and five-scale configurations offer marginal gains, they incur substantial computational overhead with diminishing returns. Consequently, the three-scale configuration represents the optimal trade-off between performance and efficiency.

\begin{table}[!t]
\centering
\caption{Detailed Ablation Study of Scale Quantities and Window Configurations}
\label{tab:scale_config}
\begin{tabular}{lllll}
\toprule
\textbf{S} & \textbf{F} & \textbf{Params} & \textbf{Train Time} & \textbf{mIoU (\%)} \\
\midrule
1 & (1) & 1.0$\times$ & 35h & 72.4 \\
2 & (1,2) & 1.13$\times$ & 37h & 73.5 \\
2 & (1,3) & 1.13$\times$ & 38h & 73.8 \\
2 & (1,4) & 1.13$\times$ & 38h & 73.6 \\
3 & (1,2,2) & 1.20$\times$ & 40h & 75.2 \\
3 & (1,2,3) & 1.20$\times$ & 41h & 75.3 \\
3 & (1,2,4) & 1.20$\times$ & 41h & 75.0 \\
3 & (1,3,3) & 1.20$\times$ & 40h & 74.8 \\
4 & (1,2,2,2) & 1.27$\times$ & 43h & 75.5 \\
4 & (1,2,2,3) & 1.27$\times$ & 44h & 75.6 \\
5 & (1,2,2,2,3) & 1.33$\times$ & 46h & 75.5 \\
\bottomrule
\end{tabular}
\par\vspace{3pt}
\footnotesize Notes: S: Number of scales; F: Scaling factors for window expansion; Params: Relative parameter count; Train Time: Total training duration on the S3DIS dataset.
\end{table}

\subsection{Qualitative Experiment}

\cref{fig:vis} presents a qualitative comparison on the S3DIS dataset, where PointSS demonstrates superior segmentation continuity and precision across complex scenes. For large-scale structures (e.g., windows and columns), the method overcomes serialization-induced discontinuities by leveraging GGAM to recover spatial relationships and ASD-SSM to maintain global context, thereby ensuring structural integrity where baselines often fail. In fine-grained regions such as clutter, the multi-scale architecture effectively captures subtle local variations, while geometric priors enable high-fidelity modeling. Additionally, at sharp boundaries (e.g., wall-column intersections), explicit neighborhood modeling mitigates fragmentation, producing distinct edges compared to the blurred boundaries of baseline methods. Overall, these visualization results align with the quantitative analysis, validating the robustness of PointSS across varying scales and geometric complexities.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\linewidth]{../picture/visualize.JPG}
	\caption{Qualitative comparison of semantic segmentation results on the S3DIS dataset.}
	\label{fig:vis}
\end{figure}

\section{Conclusion}

This paper presents PointSS, a novel framework designed to address the geometric degradation and scale inconsistency limitations of State Space Models (SSMs) in point cloud analysis. By integrating the Global Geometry-Aware Mechanism (GGAM), the spatial information loss caused by serialization is effectively compensated for through dual-sequence interaction. Furthermore, the proposed Adaptive Scale-Decoupled State Space Model (ASD-SSM) enables dynamic state decay modeling, successfully balancing local detail capture with global context learning. Extensive experiments on S3DIS and ModelNet40 benchmarks demonstrate that PointSS outperforms existing state-of-the-art methods, exhibiting particular robustness in handling complex boundaries and fragmented geometric structures. This work confirms the potential of combining explicit geometric priors with efficient sequence modeling, providing a solid foundation for future scalable 3D vision applications.

\bibliographystyle{IEEEtran}
\bibliography{../ref}

\end{document}
