

@inproceedings{pointnet ,
	title = {{PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation}},
	booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
	author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
	volume = {2017-January},
	year = {2017},
	pages = {77 - 85},
} 

@inproceedings{lora ,
	title = {{LoRA: Low-Rank Adaptation of Large Language Models}},
	booktitle = {ICLR 2022 - 10th International Conference on Learning Representations},
	author = {Hu, Edward and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	year = {2022},
	pages = {1 - 17},
} 

@inproceedings{ModelNet40 ,
	title = {{3D ShapeNets: A Deep Representation for Volumetric Shapes}},
	booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Wu, Zhirong and Song, Shuran and Khosla, Aditya and Yu, Fisher and Zhang, Linguang and Tang, Xiaoou and Xiao, Jianxiong},
	volume = {07-12-June-2015},
	year = {2015},
	pages = {1912 - 1920},
	issn = {10636919},
} 
@inproceedings{attention,
	title = {{Attention Is All You Need}},
	booktitle = {Advances in Neural Information Processing Systems 30 (NIPS 2017)},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	volume = {2017-December},
	year = {2017},
	pages = {5998 - 6008},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}
}



@inproceedings{PointNet++ ,
	title = {{PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space}},
	booktitle = {Advances in Neural Information Processing Systems},
	author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},
	volume = {2017-December},
	year = {2017},
	pages = {5100 - 5109},
} 

@inproceedings{pt ,
	title = {{Point Transformer}},
	booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
	author = {Zhao, Hengshuang and Jiang, Li and Jia, Jiaya and Torr, Philip and Koltun, Vladlen},
	year = {2021},
	pages = {16239 - 16248},
	issn = {15505499},
} 


@article{PCT ,
	title = {{PCT: Point Cloud Transformer}},
	journal = {Computational Visual Media},
	author = {Guo, Menghao and Cai, Junxiong and Liu, Zhengning and Mu, Taijiang and Martin, Ralph R. and Hu, Shimin},
	volume = {7},
	number = {2},
	year = {2021},
	pages = {187 - 199},
	issn = {20960433},
} 

	
@inproceedings{pointmae ,
	title = {{Masked Autoencoders for Point Cloud Self-Supervised Learning}},
	booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Pang, Yatian and Wang, Wenxiao and Tay, Francis E. H. and Liu, Wei and Tian, Yonghong and Yuan, Li},
	volume = {13662 LNCS},
	year = {2022},
	pages = {604 - 621},
	issn = {03029743},
} 

@inproceedings{spt ,
	title = {{Efficient 3D Semantic Segmentation with Superpoint Transformer}},
	booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
	author = {Robert, Damien and Raguet, Hugo and Landrieu, Loic},
	year = {2023},
	pages = {17149 - 17158},
	issn = {15505499},
} 
@inproceedings{OneFormer3D ,
	title = {{OneFormer3D: One Transformer for Unified Point Cloud Segmentation}},
	booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Kolodiazhnyi, Maxim and Vorontsova, Anna and Konushin, Anton and Rukhovich, Danila},
	year = {2024},
	pages = {20943 - 20953},
	issn = {10636919},
} 
@article{gsrnet,
	title = {{Robust Point Cloud Registration Using Geometric Spatial Refinement}},
	journal = {IEEE Robotics and Automation Letters},
	author = {Qi, Zhongdong and Li, Shuang and Hu, Jianwei and Chen, Xin and Wang, Zhaobin and Zhang, Xin},
	volume = {8},
	number = {7},
	year = {2023},
	pages = {4183 - 4190},
	issn = {2377-3766},
	doi = {10.1109/LRA.2023.3280817},
	publisher = {IEEE}
}


@article{geotransformer,
	title = {{GeoTransformer: Fast and Robust Point Cloud Registration with Geometric Transformer}},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Qin, Zheng and Yu, Hao and Wang, Changjian and Guo, Yulan and Peng, Yuxing and Xu, Kai},
	volume = {45},
	number = {9},
	year = {2023},
	pages = {9806 - 9821},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2023.3259038},
	publisher = {IEEE}
}
@article{pointmsgt,
	title = {{Multi-Scale Geometric Feature Extraction and Global Transformer for Real-World Indoor Point Cloud Analysis}},
	journal = {Mathematics},
	author = {Chen, Yiming and Xiao, Yiqun and Wu, Hao and Chen, Chao and Lin, Dong},
	volume = {12},
	number = {23},
	year = {2024},
	pages = {3827},
	issn = {2227-7390},
	doi = {10.3390/math12233827},
	url = {https://doi.org/10.3390/math12233827},
	publisher = {MDPI}
}
@article{pointga,
	title = {{Geometrically Aware Transformer for Point Cloud Analysis}},
	journal = {Scientific Reports},
	author = {Chen, Siyuan and Fang, Zhiwei and Wan, Siyao and Zhou, Ting and Chen, Chunlin and Wang, Meng and Li, Qianming},
	volume = {15},
	number = {1},
	year = {2025},
	pages = {16545},
	issn = {2045-2322},
	doi = {10.1038/s41598-025-00789-7},
	url = {https://doi.org/10.1038/s41598-025-00789-7},
	publisher = {Nature Publishing Group}
}	
	
@inproceedings{oacnn ,
	title = {{OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation}},
	booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Peng, Bohao and Wu, Xiaoyang and Jiang, Li and Chen, Yukang and Zhao, Hengshuang and Tian, Zhuotao and Jia, Jiaya},
	year = {2024},
	pages = {21305 - 21315},
	issn = {10636919},
} 
	
 
@inproceedings{ppt ,
	title = {{Towards Large-Scale 3D Representation Learning with Multi-Dataset Point Prompt Training}},
	booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Wu, Xiaoyang and Tian, Zhuotao and Wen, Xin and Peng, Bohao and Liu, Xihui and Yu, Kaicheng and Zhao, Hengshuang},
	year = {2024},
	pages = {19551 - 19562},
	issn = {10636919},
} 
	
	
@article{Swin3D ,
	title = {{Swin3D: A Pretrained Transformer Backbone for 3D Indoor Scene Understanding}},
	journal = {Computational Visual Media},
	author = {Yang, Yuqi and Guo, Yuxiao and Xiong, Jianyu and Liu, Yang and Pan, Hao and Wang, Pengshuai and Tong, Xin and Guo, Baining},
	volume = {11},
	number = {1},
	year = {2025},
	pages = {83 - 101},
	issn = {20960433},
	url = {http://dx.doi.org/10.26599/CVM.2025.9450383},
} 
	
@inproceedings{pmb ,
	title = {{Meta Architecture for Point Cloud Analysis}},
	booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Lin, Haojia and Zheng, Xiawu and Li, Lijiang and Chao, Fei and Wang, Shanshan and Wang, Yan and Tian, Yonghong and Ji, Rongrong},
	volume = {2023-June},
	year = {2023},
	pages = {17682 - 17691},
	issn = {10636919},
	url = {http://dx.doi.org/10.1109/CVPR52729.2023.01696},
} 
	
	
	
	
	
@inproceedings{ptv2 ,
	title = {{Point Transformer V2: Grouped Vector Attention and Partition-based Pooling}},
	booktitle = {Advances in Neural Information Processing Systems},
	author = {Wu, Xiaoyang and Lao, Yixing and Jiang, Li and Liu, Xihui and Zhao, Hengshuang},
	volume = {35},
	year = {2022},
	pages = {33330 - 33342},
	issn = {10495258},
} 

@inproceedings{ptv3 ,
	title = {{Point Transformer V3: Simpler, Faster, Stronger}},
	booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Wu, Xiaoyang and Jiang, Li and Wang, Pengshuai and Liu, Zhijian and Liu, Xihui and Qiao, Yu and Ouyang, Wanli and He, Tong and Zhao, Hengshuang},
	year = {2024},
	pages = {4840 - 4851},
	issn = {10636919},
	url = {http://dx.doi.org/10.1109/CVPR52733.2024.00463},
} 	

	
@inproceedings{Point-BERT ,
	title = {{Point-BERT: Pre-Training 3D Point Cloud Transformers with Masked Point Modeling}},
	booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Yu, Xumin and Tang, Lulu and Rao, Yongming and Huang, Tiejun and Zhou, Jie and Lu, Jiwen},
	volume = {2022-June},
	year = {2022},
	pages = {19291 - 19300},
	issn = {10636919},
} 

	
@inproceedings{ssm ,
	title = {{Efficiently Modeling Long Sequences with Structured State Spaces}},
	booktitle = {ICLR 2022 - 10th International Conference on Learning Representations},
	author = {Gu, Albert and Goel, Karan and Re, Christopher},
	year = {2022},
	pages = {1370 - 1382},
}
	
@article{RNN ,
	title = {{Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network}},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Sherstinsky, Alex},
	volume = {404},
	year = {2020},
	issn = {01672789},
	pages = {132306 - 132349},
} 
	
	
	
	
@article{Lstm ,
	title = {{A Review of Recurrent Neural Networks: LSTM Cells and Network Architectures}},
	journal = {Neural Computation},
	author = {Yu, Yong and Si, Xiaosheng and Hu, Changhua and Zhang, Jianxun},
	volume = {31},
	number = {7},
	year = {2019},
	pages = {1235 - 1270},
	issn = {08997667},
} 
	
	
	
	
@article{OctFormer ,
	title = {{Octformer: Octree-based Transformers for 3D Point Clouds}},
	journal = {ACM Transactions on Graphics},
	author = {Wang, Pengshuai},
	volume = {42},
	number = {4},
	year = {2023},
	issn = {07300301},
	pages = {5166 - 5175},
} 
	
	
	
	

@inproceedings{ConDaFormer ,
	title = {{ConDaFormer: Disassembled Transformer with Local Structure Enhancement for 3D Point Cloud Understanding}},
	booktitle = {Advances in Neural Information Processing Systems},
	author = {Duan, Lunhao and Zhao, Shanshan and Xue, Nan and Gong, Mingming and Xia, Guisong and Tao, Dacheng},
	volume = {36},
	year = {2023},
	issn = {10495258},
	pages = {23886 - 23901},
} 	
	
	
	
	
	
@article{Mamba ,
	title = {{Mamba: Linear-Time Sequence Modeling with Selective State Spaces}},
	journal = {arXiv preprint arXiv:2312.00752},
	author = {Gu, Albert and Dao, Tri},
	year = {2023},
	issn = {23318422},
	url = {http://dx.doi.org/10.48550/arXiv.2312.00752},
} 
	
@inproceedings{superpoint ,
	title = {{Superpoint Transformer for 3D Scene Instance Segmentation}},
	booktitle = {Proceedings of the 37th AAAI Conference on Artificial Intelligence, AAAI 2023},
	author = {Sun, Jiahao and Qing, Chunmei and Tan, Junpeng and Xu, Xiangmin},
	volume = {37},
	year = {2023},
	pages = {2393 - 2401},
} 
	
	
	
@inproceedings{s3dis ,
	title = {{3D Semantic Parsing of Large-Scale Indoor Spaces}},
	booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Armeni, Iro and Sener, Ozan and Zamir, Amir R. and Jiang, Helen and Brilakis, Ioannis and Fischer, Martin and Savarese, Silvio},
	volume = {2016-December},
	year = {2016},
	pages = {1534 - 1543},
	issn = {10636919},
} 


	
@inproceedings{PointMamba ,
	title = {{PointMamba: A Simple State Space Model for Point Cloud Analysis}},
	booktitle = {Advances in Neural Information Processing Systems},
	author = {Liang, Dingkang and Zhou, Xin and Xu, Wei and Zhu, Xingkui and Zou, Zhikang and Ye, Xiaoqing and Tan, Xiao and Bai, Xiang},
	volume = {37},
	year = {2024},
	issn = {10495258},
	pages = {1534 - 1543},
} 

@inproceedings{pcm,
	title = {{Point Cloud Mamba: Point Cloud Learning via State Space Model}},
	booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhang, Tao and Yuan, Haobo and Qi, Lu and Zhang, Jiangning and Zhou, Qianyu and Ji, Shunping and Yan, Shuicheng and Li, Xiangtai},
	volume = {39},
	number = {10},
	year = {2025},
	pages = {10121 - 10130},
	URL = {http://dx.doi.org/10.1609/aaai.v39i10.33098},
}

@article{MSDCNN ,
	title = {{MSDCNN: A Multiscale Dilated Convolution Neural Network for Fine-Grained 3D Shape Classification}},
	journal = {Neural Networks},
	author = {Zhou, Wei and Zheng, Fujian and Zhao, Yiheng and Pang, Yiran and Yi, Jun},
	volume = {172},
	year = {2024},
	issn = {08936080},
	pages = {106141 - 106151},
} 
@article{C2BG ,
	title = {{C2BG-Net: Cross-Modality and Cross-Scale Balance Network with Global Semantics for Multi-Modal 3D Object Detection}},
	journal = {Neural Networks},
	author = {Ding, Bonan and Xie, Jin and Nie, Jing and Wu, Yulong and Cao, Jiale},
	volume = {179},
	year = {2024},
	issn = {08936080},
	pages = {106535 - 106546},
} 
@article{Vertex,
	title = {{Vertex Points Are Not Enough: Monocular 3D Object Detection via Intra- and Inter-plane Constraints}},
	journal = {Neural Networks},
	volume = {162},
	pages = {350 - 358},
	year = {2023},
	issn = {0893-6080},
	doi = {10.1016/j.neunet.2023.02.038},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608023001119},
	author = {Yao, Hongdou and Chen, Jun and Wang, Zheng and Wang, Xiao and Chai, Xiaoyu and Qiu, Yansheng and Han, Pengfei},
}

@inproceedings{VisionMamba ,
	title = {{Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model}},
	booktitle = {Proceedings of Machine Learning Research},
	author = {Zhu, Lianghui and Liao, Bencheng and Zhang, Qian and Wang, Xinlong and Liu, Wenyu and Wang, Xinggang},
	volume = {235},
	year = {2024},
	pages = {62429 - 62442},
	issn = {26403498},
} 	
	
@article{Pyramid ,
	title = {{Pyramid Scene Parsing Network in 3D: Improving Semantic Segmentation of Point Clouds with Multi-Scale Contextual Information}},
	journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
	author = {Fang, Hao and Lafarge, Florent},
	volume = {154},
	year = {2019},
	pages = {246 - 258},
	issn = {09242716},
	url = {http://dx.doi.org/10.1016/j.isprsjprs.2019.06.010},
} 
	
	
@inproceedings{ScanNet ,
	title = {{ScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes}},
	booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
	author = {Dai, Angela and Chang, Angel X. and Savva, Manolis and Halber, Maciej and Funkhouser, Thomas and Niecner, Matthias},
	volume = {2017-January},
	year = {2017},
	pages = {2432 - 2443},
	url = {http://dx.doi.org/10.1109/CVPR.2017.261},
} 

	
@inproceedings{KPConv ,
	title = {{KPConv: Flexible and Deformable Convolution for Point Clouds}},
	booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
	author = {Thomas, Hugues and Qi, Charles R. and Marcotegui, Beatriz and Goulette, Francois and Guibas, Leonidas},
	year = {2019},
	pages = {6410 - 6419},
	issn = {15505499},
	url = {http://dx.doi.org/10.1109/ICCV.2019.00651},
} 	


@inproceedings{Randla ,
	title = {{RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds}},
	booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Hu, Qingyong and Yang, Bo and Xie, Linhai and Rosa, Stefano and Guo, Yulan and Wang, Zhihua and Trigoni, Niki and Markham, Andrew},
	year = {2020},
	pages = {11105 - 11114},
	issn = {10636919},
	url = {http://dx.doi.org/10.1109/CVPR42600.2020.01112},
} 	
	
@article{FusionNet ,
	title = {{FusionNet: Detection of Foreign Objects in Transmission Lines During Inclement Weather}},
	journal = {IEEE Transactions on Instrumentation and Measurement},
	author = {Ji, Chao and Jia, Xinghai and Huang, Xinbo and Zhou, Siyuan and Chen, Guoyan and Zhu, Yongcan},
	volume = {73},
	year = {2024},
	pages = {1 - 18},
	issn = {00189456},
	url = {http://dx.doi.org/10.1109/TIM.2024.3403173},
} 
@article{DGCNN ,
	title = {{Dynamic Graph CNN for Learning on Point Clouds}},
	journal = {ACM Transactions on Graphics},
	author = {Wang, Yue and Sun, Yongbin and Liu, Ziwei and Sarma, Sanjay E. and Bronstein, Michael M. and Solomon, Justin M.},
	volume = {38},
	number = {5},
	pages = {1 - 12},
	year = {2019},
	issn = {07300301},
} 
@inproceedings{fpn,
	title = {{Feature Pyramid Networks for Object Detection}},
	author = {Lin, Tsung-Yi and Dollar, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	pages = {2117 - 2125},
	year = {2017}
}
	
@inproceedings{Point-GNN ,
	title = {{Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud}},
	booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Shi, Weijing and Rajkumar, Ragunathan},
	year = {2020},
	pages = {1708 - 1716},
	issn = {10636919},
	url = {http://dx.doi.org/10.1109/CVPR42600.2020.00178},
} 	
	
	
@inproceedings{geo-cnn ,
	title = {{Modeling Local Geometric Structure of 3D Point Clouds Using Geo-CNN}},
	booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Lan, Shiyi and Yu, Ruichi and Yu, Gang and Davis, Larry S.},
	volume = {2019-June},
	year = {2019},
	pages = {998 - 1008},
	issn = {10636919},
	url = {http://dx.doi.org/10.1109/CVPR.2019.00109},
} 
	
@inproceedings{FEMAE ,
	title = {{Towards Compact 3D Representations via Point Feature Enhancement Masked Autoencoders}},
	booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zha, Yaohua and Ji, Huizhen and Li, Jinmin and Li, Rongsheng and Dai, Tao and Chen, Bin and Wang, Zhi and Xia, Shutao},
	volume = {38},
	number = {7},
	year = {2024},
	pages = {6962 - 6970},
	issn = {21595399},
} 
	
	
@inproceedings{Point2Vec ,
	title = {{Point2Vec for Self-Supervised Representation Learning on Point Clouds}},
	booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Zeid, Karim Abou and Schult, Jonas and Hermans, Alexander and Leibe, Bastian},
	volume = {14264 LNCS},
	year = {2024},
	pages = {131 - 146},
	issn = {03029743},
	url = {http://dx.doi.org/10.1007/978-3-031-54605-1_9},
} 	


@inproceedings{Mamba3D ,
	title = {{Mamba3D: Enhancing Local Features for 3D Point Cloud Analysis via State Space Model}},
	booktitle = {MM 2024 - Proceedings of the 32nd ACM International Conference on Multimedia},
	author = {Han, Xu and Tang, Yuan and Wang, Zhaoxuan and Li, Xianzhi},
	year = {2024},
	pages = {4995 - 5004},
	url = {http://dx.doi.org/10.1145/3664647.3681173},
} 
	
@article{OTMae3D ,
	title = {{Rethinking Masked Representation Learning for 3D Point Cloud Understanding}},
	journal = {IEEE Transactions on Image Processing},
	author = {Wang, Chuxin and Zha, Yixin and He, Jianfeng and Yang, Wenfei and Zhang, Tianzhu},
	volume = {34},
	year = {2025},
	pages = {247 - 262},
	issn = {10577149},
} 
% Multi-scale feature fusion references for point cloud analysis

@article{adaptive_fusion_2025,
	title={{Multi-Scale Sparse Convolution and Point Convolution Adaptive Fusion Point Cloud Semantic Segmentation Method}},
	author={Bi, Yuxuan and Liu, Peng and Zhang, Tianyi and Shi, Jialin and Wang, Caixia},
	journal={Scientific Reports},
	volume={15},
	number={1},
	pages={2308},
	year={2025},
	publisher={Nature Publishing Group},
	doi={10.1038/s41598-025-88905-5}
}



@article{projection_based_fusion_2024,
	title={{Multi-Scale Feature Fusion Point Cloud Object Detection Based on Original Point Cloud and Projection}},
	author={Liu, Zhiyu and Zhang, Kai and Wang, Hao and Chen, Mingshan},
	journal={Electronics},
	volume={13},
	number={11},
	pages={2213},
	year={2024},
	publisher={MDPI},
	doi={10.3390/electronics13112213}
}

@article{local_feature_fusion_2024,
	title={{Point Cloud Semantic Segmentation Based on Local Feature Fusion and Multilayer Attention Network}},
	author={Wen, Chao and Li, Xiaokang and Yao, Xueming and Mu, Bowen and Zhang, Jing and Xue, Jingpei},
	journal={IET Computer Vision},
	volume={18},
	number={2},
	pages={200 - 212},
	year={2024},
	publisher={IET},
	doi={10.1049/cvi2.12255}
}

@article{multiscale_survey_2024,
	title={{Deep Learning for 3D Point Cloud Enhancement: A Survey}},
	author={Li, Siqi and Zhang, Wei and Wang, Xiaoming and Chen, Hao},
	journal={arXiv preprint arXiv:2411.00857},
	year={2024},
	url={https://arxiv.org/abs/2411.00857}
}

@article{ccmnet,
	title = {{Exploring Multi-Scale and Cross-Type Features in 3D Point Cloud Learning with CCMNet}},
	journal = {Expert Systems with Applications},
	author = {Zhou, Wei and Jin, Weiwei and Wang, Dekui and Hao, Xingxing and Yu, Yongxiang and Ma, Caiwen},
	volume = {274},
	year = {2025},
	pages = {126960},
	doi = {10.1016/j.eswa.2025.126960},
}

@article{pointstack,
	title = {{Advanced Feature Learning on Point Clouds Using Multi-Resolution Features and Learnable Pooling}},
	journal = {Remote Sensing},
	author = {Wijaya, Kevin Tirta and Paek, Dong-Hee and Kong, Seung-Hyun},
	volume = {16},
	number = {11},
	year = {2024},
	pages = {1835},
	doi = {10.3390/rs16111835},
}

@inproceedings{20224813183060 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2025 Elsevier Inc.},
copyright = {Compendex},
title = {2DPASS: 2D Priors Assisted Semantic Segmentation on LiDAR Point Clouds},
journal = {Lecture Notes in Computer Science},
author = {Yan, Xu and Gao, Jiantao and Zheng, Chaoda and Zheng, Chao and Zhang, Ruimao and Cui, Shuguang and Li, Zhen},
volume = {13688 LNCS},
year = {2022},
pages = {677 - 695},
issn = {03029743},
address = {Tel Aviv, Israel},
abstract = {<div data-language="eng" data-ev-field="abstract">As camera and LiDAR sensors capture complementary information in autonomous driving, great efforts have been made to conduct semantic segmentation through multi-modality data fusion. However, fusion-based approaches require paired data, i.e., LiDAR point clouds and camera images with strict point-to-pixel mappings, as the inputs in both training and inference stages. It seriously hinders their application in practical scenarios. Thus, in this work, we propose the 2D Priors Assisted Semantic Segmentation (2DPASS) method, a general training scheme, to boost the representation learning on point clouds. The proposed 2DPASS method fully takes advantage of 2D images with rich appearance during training, and then conduct semantic segmentation without strict paired data constraints. In practice, by leveraging an auxiliary modal fusion and multi-scale fusion-to-single knowledge distillation (MSFSKD), 2DPASS acquires richer semantic and structural information from the multi-modal data, which are then distilled to the pure 3D network. As a result, our baseline model shows significant improvement with only point cloud inputs once equipped with the 2DPASS. Specifically, it achieves the state-of-the-arts on two large-scale recognized benchmarks (i.e., SemanticKITTI and NuScenes), i.e., ranking the top-1 in both single and multiple scan(s) competitions of SemanticKITTI.<br/></div> © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
key = {Distillation},
keywords = {Cameras;Data fusion;Knowledge management;Modal analysis;Optical radar;Semantic Segmentation;Semantics;},
note = {Autonomous driving;Camera images;Cloud image;Knowledge distillation;LiDAR point cloud;Multi-modal;Multi-modality;Point-clouds;Segmentation methods;Semantic segmentation;},
URL = {http://dx.doi.org/10.1007/978-3-031-19815-1_39},
}

@unpublished{20220181093 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2025 Elsevier Inc.},
copyright = {Compendex},
title = {PointNeXt: Revisiting PointNet++ with Improved Training and Scaling Strategies},
journal = {arXiv},
author = {Qian, Guocheng and Li, Yuchen and Peng, Houwen and Mai, Jinjie and Hammoud, Hasan Abed Al Kader and Elhoseiny, Mohamed and Ghanem, Bernard},
year = {2022},
issn = {23318422},
abstract = {<div data-language="eng" data-ev-field="abstract">PointNet++ is one of the most influential neural architectures for point cloud understanding. Although the accuracy of PointNet++ has been largely surpassed by recent networks such as PointMLP and Point Transformer, we find that a large portion of the performance gain is due to improved training strategies, i.e. data augmentation and optimization techniques, and increased model sizes rather than architectural innovations. Thus, the full potential of PointNet++ has yet to be explored. In this work, we revisit the classical PointNet++ through a systematic study of model training and scaling strategies, and offer two major contributions. First, we propose a set of improved training strategies that significantly improve PointNet++ performance. For example, we show that, without any change in architecture, the overall accuracy (OA) of PointNet++ on ScanObjectNN object classification can be raised from 77.9% to 86.1%, even outperforming state-of-the-art PointMLP. Second, we introduce an inverted residual bottleneck design and separable MLPs into PointNet++ to enable efficient and effective model scaling and propose PointNeXt, the next version of PointNets. PointNeXt can be flexibly scaled up and outperforms state-of-the-art methods on both 3D classification and segmentation tasks. For classification, PointNeXt reaches an overall accuracy of 87.7% on ScanObjectNN, surpassing PointMLP by 2.3%, while being 10× faster in inference. For semantic segmentation, PointNeXt establishes a new state-of-the-art performance with 74.9% mean IoU on S3DIS (6-fold cross-validation), being superior to the recent Point Transformer. The code and models are available at https://github.com/guochengqian/pointnext.<br/></div> © 2022, CC BY-NC-SA.},
key = {Semantic Segmentation},
keywords = {Architecture;Computer vision;Network architecture;Semantics;},
note = {Augmentation techniques;Data augmentation;Data optimization;Neural architectures;Optimization techniques;Overall accuracies;Performance Gain;Point-clouds;Scalings;Training strategy;},
URL = {http://dx.doi.org/10.48550/arXiv.2206.04670},
}

@inproceedings{20224613119817 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2025 Elsevier Inc.},
copyright = {Compendex},
title = {Fast Point Transformer},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
author = {Park, Chunghyun and Jeong, Yoonwoo and Cho, Minsu and Park, Jaesik},
volume = {2022-June},
year = {2022},
pages = {16928 - 16937},
issn = {10636919},
address = {New Orleans, LA, United states},
abstract = {<div data-language="eng" data-ev-field="abstract">The recent success of neural networks enables a better interpretation of 3D point clouds, but processing a large-scale 3D scene remains a challenging problem. Most current approaches divide a large-scale scene into small regions and combine the local predictions together. However, this scheme inevitably involves additional stages for pre- and post-processing and may also degrade the final output due to predictions in a local perspective. This paper introduces Fast Point Transformer that consists of a new lightweight self-attention layer. Our approach encodes continuous 3D coordinates, and the voxel hashing-based architecture boosts computational efficiency. The proposed method is demonstrated with 3D semantic segmentation and 3D detection. The accuracy of our approach is competitive to the best voxel-based method, and our network achieves 129 times faster inference time than the state-of-the-art, Point Transformer, with a reasonable accuracy trade-off in 3D semantic segmentation on S3DIS dataset.<br/></div> © 2022 IEEE.},
URL = {http://dx.doi.org/10.1109/CVPR52688.2022.01644},
}

@inproceedings{20224613119724 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2025 Elsevier Inc.},
copyright = {Compendex},
title = {Stratified Transformer for 3D Point Cloud Segmentation},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
author = {Lai, Xin and Liu, Jianhui and Jiang, Li and Wang, Liwei and Zhao, Hengshuang and Liu, Shu and Qi, Xiaojuan and Jia, Jiaya},
volume = {2022-June},
year = {2022},
pages = {8490 - 8499},
issn = {10636919},
address = {New Orleans, LA, United states},
abstract = {<div data-language="eng" data-ev-field="abstract">3D point cloud segmentation has made tremendous progress in recent years. Most current methods focus on aggregating local features, but fail to directly model long-range dependencies. In this paper, we propose Stratified Transformer that is able to capture long-range contexts and demonstrates strong generalization ability and high performance. Specifically, we first put forward a novel key sampling strategy. For each query point, we sample nearby points densely and distant points sparsely as its keys in a stratified way, which enables the model to enlarge the effective receptive field and enjoy long-range contexts at a low computational cost. Also, to combat the challenges posed by irregular point arrangements, we propose first-layer point embedding to aggregate local information, which facilitates convergence and boosts performance. Besides, we adopt contextual relative position encoding to adaptively capture position information. Finally, a memory-efficient implementation is introduced to overcome the issue of varying point numbers in each window. Extensive experiments demonstrate the effectiveness and superiority of our method on S3DIS, ScanNetv2 and ShapeNetPart datasets. Code is available at https://github.com/dvlab-research/Stratified-Transformer.<br/></div> © 2022 IEEE.},
URL = {http://dx.doi.org/10.1109/CVPR52688.2022.00831},
}

@inproceedings{20234114867303 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2025 Elsevier Inc.},
copyright = {Compendex},
title = {Spherical Transformer for LiDAR-Based 3D Recognition},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
author = {Lai, Xin and Chen, Yukang and Lu, Fanbin and Liu, Jianhui and Jia, Jiaya},
volume = {2023-June},
year = {2023},
pages = {17545 - 17555},
issn = {10636919},
address = {Vancouver, BC, Canada},
abstract = {<div data-language="eng" data-ev-field="abstract">LiDAR-based 3D point cloud recognition has benefited various applications. Without specially considering the LiDAR point distribution, most current methods suffer from information disconnection and limited receptive field, especially for the sparse distant points. In this work, we study the varying-sparsity distribution of LiDAR points and present SphereFormer to directly aggregate information from dense close points to the sparse distant ones. We design radial window self-attention that partitions the space into multiple non-overlapping narrow and long windows. It overcomes the disconnection issue and enlarges the receptive field smoothly and dramatically, which significantly boosts the performance of sparse distant points. Moreover, to fit the narrow and long windows, we propose exponential splitting to yield fine-grained position encoding and dynamic feature selection to increase model representation ability. Notably, our method ranks 1st on both nuScenes and SemanticKITTI semantic segmentation benchmarks with 81.9% and 74.8% mIoU, respectively. Also, we achieve the 3rd place on nuScenes object detection benchmark with 72.8% NDS and 68.5% mAP. Code is available at https://github.com/dvlab-research/SphereFormer.git.<br/></div> © 2023 IEEE.},
URL = {http://dx.doi.org/10.1109/CVPR52729.2023.01683},
}

@inproceedings{20220611608729 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2025 Elsevier Inc.},
copyright = {Compendex},
title = {PAConv: Position Adaptive Convolution with Dynamic Kernel Assembling on Point Clouds},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
author = {Xu, Mutian and Ding, Runyu and Zhao, Hengshuang and Qi, Xiaojuan},
year = {2021},
pages = {3172 - 3181},
issn = {10636919},
address = {Virtual, Online, United states},
abstract = {<div data-language="eng" data-ev-field="abstract">We introduce Position Adaptive Convolution (PAConv), a generic convolution operation for 3D point cloud processing. The key of PAConv is to construct the convolution kernel by dynamically assembling basic weight matrices stored in Weight Bank, where the coefficients of these weight matrices are self-adaptively learned from point positions through ScoreNet. In this way, the kernel is built in a data-driven manner, endowing PAConv with more flexibility than 2D convolutions to better handle the irregular and unordered point cloud data. Besides, the complexity of the learning process is reduced by combining weight matrices instead of brutally predicting kernels from point positions. Furthermore, different from the existing point convolution operators whose network architectures are often heavily engineered, we integrate our PAConv into classical MLP-based point cloud pipelines without changing network configurations. Even built on simple networks, our method still approaches or even surpasses the state-of-the-art models, and significantly improves baseline performance on both classification and segmentation tasks, yet with decent efficiency. Thorough ablation studies and visualizations are provided to understand PAConv. Code is released on https://github.com/CVMI-Lab/PAConv.<br/></div> © 2021 IEEE},
key = {Convolution},
keywords = {Computer vision;Network architecture;Matrix algebra;},
note = {2-D convolution;3D point cloud;Cloud processing;Convolution kernel;Data driven;Learning process;Point cloud data;Point-clouds;Unordered point cloud;Weight matrices;},
URL = {http://dx.doi.org/10.1109/CVPR46437.2021.00319},
}

@inproceedings{20250817913314 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2025 Elsevier Inc.},
copyright = {Compendex},
title = {Learning 3D Representations from 2D Pre-Trained Models via Image-to-Point Masked Autoencoders},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
author = {Zhang, Renrui and Wang, Liuhui and Qiao, Yu and Gao, Peng and Li, Hongsheng},
volume = {2023-June},
year = {2023},
pages = {21769 - 21780},
issn = {10636919},
address = {Vancouver, BC, Canada},
abstract = {<div data-language="eng" data-ev-field="abstract">Pre-training by numerous image data has become defacto for robust 2D representations. In contrast, due to the expensive data processing, a paucity of 3D datasets severely hinders the learning for high-quality 3D features. In this paper, we propose an alternative to obtain superior 3D representations from 2D pre-trained models via Image-to-Point Masked Autoencoders, named as I2P-MAE. By self-supervised pre-training, we leverage the well learned 2D knowledge to guide 3D masked autoencoding, which reconstructs the masked point tokens with an encoder-decoder architecture. Specifically, we first utilize off-the-shelf 2D models to extract the multi-view visual features of the input point cloud, and then conduct two types of image-to-point learning schemes. For one, we introduce a 2D-guided masking strategy that maintains semantically important point tokens to be visible. Compared to random masking, the network can better concentrate on significant 3D structures with key spatial cues. For another, we enforce these visible tokens to reconstruct multi-view 2D features after the decoder. This enables the network to effectively inherit high-level 2D semantics for discriminative 3D modeling. Aided by our image-to-point pre-training, the frozen I2P-MAE, without any fine-tuning, achieves 93.4% accuracy for linear SVM on ModelNet40, competitive to existing fully trained methods. By further fine-tuning on on ScanObjectNN's hardest split, I2P-MAE attains the state-of-the-art 90.11% accuracy, +3.68% to the second-best, demonstrating superior transferable capacity. Code is available at https://github.com/ZrrSkywalker/I2P-MAE.<br/></div> © 2023 IEEE.},
key = {Contrastive Learning},
keywords = {Adversarial machine learning;Image coding;Network security;Self-supervised learning;Signal encoding;Supervised learning;},
note = {3d from multi-view and sensor;3d representations;Auto encoders;Encoder-decoder architecture;Fine tuning;High quality;Image data;Multi sensor;Multi-views;Pre-training;},
URL = {http://dx.doi.org/10.1109/CVPR52729.2023.02085},
}


@inproceedings{20232614296126 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2025 Elsevier Inc.},
copyright = {Compendex},
title = {Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training},
journal = {Advances in Neural Information Processing Systems},
author = {Zhang, Renrui and Guo, Ziyu and Fang, Rongyao and Zhao, Bin and Wang, Dong and Qiao, Yu and Li, Hongsheng and Gao, Peng},
volume = {35},
year = {2022},
issn = {10495258},
address = {New Orleans, LA, United states},
abstract = {<div data-language="eng" data-ev-field="abstract">Masked Autoencoders (MAE) have shown great potentials in self-supervised pretraining for language and 2D image transformers. However, it still remains an open question on how to exploit masked autoencoding for learning 3D representations of irregular point clouds. In this paper, we propose Point-M2AE, a strong Multi-scale MAE pre-training framework for hierarchical self-supervised learning of 3D point clouds. Unlike the standard transformer in MAE, we modify the encoder and decoder into pyramid architectures to progressively model spatial geometries and capture both fine-grained and high-level semantics of 3D shapes. For the encoder that downsamples point tokens by stages, we design a multi-scale masking strategy to generate consistent visible regions across scales, and adopt a local spatial self-attention mechanism during fine-tuning to focus on neighboring patterns. By multi-scale token propagation, the lightweight decoder gradually upsamples point tokens with complementary skip connections from the encoder, which further promotes the reconstruction from a global-to-local perspective. Extensive experiments demonstrate the state-of-the-art performance of Point-M2AE for 3D representation learning. With a frozen encoder after pretraining, Point-M2AE achieves 92.9% accuracy for linear SVM on ModelNet40, even surpassing some fully trained methods. By fine-tuning on downstream tasks, Point-M2AE achieves 86.43% accuracy on ScanObjectNN, +3.36% to the second-best, and largely benefits the few-shot classification, part segmentation and 3D object detection with the hierarchical pre-training scheme. Code is available at https://github.com/ZrrSkywalker/Point-M2AE.<br/></div> © 2022 Neural information processing systems foundation. All rights reserved.},
key = {Object detection},
keywords = {Backpropagation;Decoding;Learning systems;Semantics;Signal encoding;Support vector machines;},
note = {2D images;3D point cloud;3d representations;Auto encoders;Fine tuning;Image transformers;Multi-scales;Point-clouds;Pre-training;Training framework;},
}

@inproceedings{20241715986616 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2025 Elsevier Inc.},
copyright = {Compendex},
title = {PointGPT: Auto-regressively Generative Pre-training from Point Clouds},
journal = {Advances in Neural Information Processing Systems},
author = {Chen, Guangyan and Wang, Meiling and Yang, Yi and Yu, Kai and Yuan, Li and Yue, Yufeng},
volume = {36},
year = {2023},
issn = {10495258},
address = {New Orleans, LA, United states},
abstract = {<div data-language="eng" data-ev-field="abstract">Large language models (LLMs) based on the generative pre-training transformer (GPT) [46] have demonstrated remarkable effectiveness across a diverse range of downstream tasks. Inspired by the advancements of the GPT, we present PointGPT, a novel approach that extends the concept of GPT to point clouds, addressing the challenges associated with disorder properties, low information density, and task gaps. Specifically, a point cloud auto-regressive generation task is proposed to pre-train transformer models. Our method partitions the input point cloud into multiple point patches and arranges them in an ordered sequence based on their spatial proximity. Then, an extractor-generator based transformer decoder [27], with a dual masking strategy, learns latent representations conditioned on the preceding point patches, aiming to predict the next one in an auto-regressive manner. To explore scalability and enhance performance, a larger pre-training dataset is collected. Additionally, a subsequent post-pre-training stage is introduced, incorporating a labeled hybrid dataset. Our scalable approach allows for learning high-capacity models that generalize well, achieving state-of-the-art performance on various downstream tasks. In particular, our approach achieves classification accuracies of 94.9% on the ModelNet40 dataset and 93.4% on the ScanObjectNN dataset, outperforming all other transformer models. Furthermore, our method also attains new state-of-the-art accuracies on all four few-shot learning benchmarks. Codes are available at https://github.com/CGuangyan-BIT/PointGPT.<br/></div> © 2023 Neural information processing systems foundation. All rights reserved.},
key = {Classification (of information)},
note = {Auto-regressive;Diverse range;Down-stream;Information density;Language model;Model-based OPC;Point-clouds;Pre-training;Property;Transformer modeling;},
}


@inproceedings{20224613119405 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2025 Elsevier Inc.},
copyright = {Compendex},
title = {PointCLIP: Point Cloud Understanding by CLIP},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
author = {Zhang, Renrui and Guo, Ziyu and Zhang, Wei and Li, Kunchang and Miao, Xupeng and Cui, Bin and Qiao, Yu and Gao, Peng and Li, Hongsheng},
volume = {2022-June},
year = {2022},
pages = {8542 - 8552},
issn = {10636919},
address = {New Orleans, LA, United states},
abstract = {<div data-language="eng" data-ev-field="abstract">Recently, zero-shot and few-shot learning via Contrastive Vision-Language Pre-training (CLIP) have shown inspirational performance on 2D visual recognition, which learns to match images with their corresponding texts in open-vocabulary settings. However, it remains under explored that whether CLIP, pre-trained by large-scale image-text pairs in 2D, can be generalized to 3D recognition. In this paper, we identify such a setting is feasible by proposing PointCLIP, which conducts alignment between CLIP-encoded point clouds and 3D category texts. Specifically, we encode a point cloud by projecting it onto multi-view depth maps and aggregate the view-wise zero-shot prediction in an end-to-end manner, which achieves efficient knowledge transfer from 2D to 3D. We further design an inter-view adapter to better extract the global feature and adaptively fuse the 3D few-shot knowledge into CLIP pre-trained in 2D. By just fine-tuning the adapter under few-shot settings, the performance of PointCLIP could be largely improved. In addition, we observe the knowledge complementary property between PointCLIP and classical 3D-supervised networks. Via simple ensemble during inference, PointCLIP contributes to favorable performance enhancement over state-of-the-art 3D networks. Therefore, PointCLIP is a promising alternative for effective 3D point cloud understanding under low data regime with marginal resource cost. We conduct thorough experiments on Model-NetlO, ModelNet40 and ScanObjectNN to demonstrate the effectiveness of PointCLIP. Code is available at https://github.com/ZrrSkywalker/PointCLIP.<br/></div> © 2022 IEEE.},
URL = {http://dx.doi.org/10.1109/CVPR52688.2022.00836},
}

@inproceedings{20225013227515 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2025 Elsevier Inc.},
copyright = {Compendex},
title = {Surface Representation for Point Clouds},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
author = {Ran, Haoxi and Liu, Jun and Wang, Chengjie},
volume = {2022-June},
year = {2022},
pages = {18920 - 18930},
issn = {10636919},
address = {New Orleans, LA, United states},
abstract = {<div data-language="eng" data-ev-field="abstract">Most prior work represents the shapes of point clouds by coordinates. However, it is insufficient to describe the local geometry directly. In this paper, we present RepSurf (representative surfaces), a novel representation of point clouds to explicitly depict the very local structure. We explore two variants of RepSurf, Triangular RepSurf and Umbrella RepSurf inspired by triangle meshes and umbrella curvature in computer graphics. We compute the representations of RepSurf by predefined geometric priors after surface reconstruction. RepSurf can be a plug-and-play module for most point cloud models thanks to its free collaboration with irregular points. Based on a simple baseline of PointNet++ (SSG version), Umbrella RepSurf surpasses the previous state-of-the-art by a large margin for classification, segmentation and detection on various benchmarks in terms of performance and efficiency. With an increase of around 0.008M number of parameters, 0.04G FLOPs, and 1.12ms inference time, our method achieves 94.7% (+0.5%) on ModelNet40, and 84.6% (+1.8%) on ScanObjectNN for classification, while 74.3% (+0.8%) mIoU on S3DIS 6-fold, and 70.0% (+1.6%) mIoU on ScanNet for segmentation. For detection, previous state-of-the-art detector with our RepSurf obtains 71.2% (+2.1%) mAP25, 54.8% (+2.0%) mAP50 on ScanNetV2, and 64.9% (+1.9%) mAP25, 47.7% (+2.5%) mAP50 on SUN RGB-D. Our lightweight Triangular RepSurf performs its excellence on these benchmarks as well. The code is publicly available at https://github.com/hancyran/RepSurf.<br/></div> © 2022 IEEE.},
key = {Benchmarking},
keywords = {Computer graphics;Computer vision;},
note = {Categorization;Grouping and shape analyse;Recognition: detection;Representation learning;Retrieval;RGBD sensor and analytic;Scene analysis;Scene understanding;Segmentation;Shape-analysis;},
URL = {http://dx.doi.org/10.1109/CVPR52688.2022.01837},
}

@article{20240215336176 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2025 Elsevier Inc.},
copyright = {Compendex},
title = {Enhancing LiDAR-Based Object Recognition Through a Novel Denoising and Modified GDANet Framework},
journal = {IEEE Access},
author = {Putra, Oddy Virgantara and Riansyah, Moch. Iskandar and Rahmanti, Farah Zakiyah and Priyadi, Ardyono and Wulandari, Diah Puspito and Ogata, Kohichi and Yuniarno, Eko Mulyanto and Purnomo, Mauridhi Hery},
volume = {12},
year = {2024},
pages = {7285 - 7297},
issn = {21693536},
abstract = {<div data-language="eng" data-ev-field="abstract">Object recognition in Point Cloud data from LiDAR sensors often faces challenges like noise, clutter, and ground interference, significantly affecting tasks such as segmentation, classification, and detection. To address these issues, we introduced a framework comprising a denoiser and a classifier, enhancing the robustness of LiDAR-based object recognition. The denoiser plays a crucial role in noise mitigation and operates as a two-part system, utilizing ScoreNet and the Guided Filter. ScoreNet employs advanced scoring techniques to separate valuable information from noise, while the Guided Filter further refines the data, preserving crucial details. The output from the denoiser seamlessly feeds into the classifier, leveraging a modified GDANet architecture with depthwise overparameterized convolution (DOConv) to capture intricate features. We evaluated our approach using Point-to-Point, Hausdorff distance, and Accuracy metrics, comparing it with other denoising methods and point cloud classifiers. Our models demonstrated significant improvements in denoising and classification tasks, with the denoiser achieving outstanding results in the Hausdorff Distance metric, reaching a score of 0.177. Simultaneously, the classifier outperformed other point cloud classifiers, achieving accuracy scores of 90.7% and 96.7% for ModelNet40-C and Human Pose Dataset, respectively. These achievements underscore the importance of our framework in addressing the challenges of noise and clutter in Point Cloud data, ultimately advancing LiDAR-based object recognition.<br/></div> © 2013 IEEE.},
key = {Optical radar},
keywords = {Classification (of information);Clutter (information theory);Convolution;Geometry;Job analysis;Noise abatement;Object recognition;Radar clutter;Three dimensional displays;},
note = {Clutter;De-noising;Depthwise convolution;Human pose;Human pose classification;LiDAR;Objects recognition;Point cloud compression;Point cloud denoising;Point-clouds;Pose classifications;Task analysis;Three-dimensional display;},
URL = {http://dx.doi.org/10.1109/ACCESS.2023.3347033},
}

@article{20231413850262 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2025 Elsevier Inc.},
copyright = {Compendex},
title = {GeoTransformer: Fast and Robust Point Cloud Registration With Geometric Transformer},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
author = {Qin, Zheng and Yu, Hao and Wang, Changjian and Guo, Yulan and Peng, Yuxing and Ilic, Slobodan and Hu, Dewen and Xu, Kai},
volume = {45},
number = {8},
year = {2023},
pages = {9806 - 9821},
issn = {01628828},
abstract = {<div data-language="eng" data-ev-field="abstract">We study the problem of extracting accurate correspondences for point cloud registration. Recent keypoint-free methods have shown great potential through bypassing the detection of repeatable keypoints which is difficult to do especially in low-overlap scenarios. They seek correspondences over downsampled superpoints, which are then propagated to dense points. Superpoints are matched based on whether their neighboring patches overlap. Such sparse and loose matching requires contextual features capturing the geometric structure of the point clouds. We propose Geometric Transformer, or GeoTransformer for short, to learn geometric feature for robust superpoint matching. It encodes pair-wise distances and triplet-wise angles, making it invariant to rigid transformation and robust in low-overlap cases. The simplistic design attains surprisingly high matching accuracy such that no RANSAC is required in the estimation of alignment transformation, leading to 100 times acceleration. Extensive experiments on rich benchmarks encompassing indoor, outdoor, synthetic, multiway and non-rigid demonstrate the efficacy of GeoTransformer. Notably, our method improves the inlier ratio by $18{\sim }31$18∼31 percentage points and the registration recall by over 7 points on the challenging 3DLoMatch benchmark.<br/></div> © 1979-2012 IEEE.},
key = {Acceleration},
keywords = {Feature extraction;Geometry;Job analysis;Three dimensional displays;},
note = {Benchmark testing;Coarse to fine;Coarse-to-fine correspondence;Convergence;Features extraction;Geometric consistency;Point cloud compression;Point cloud matching;Point cloud registration;Point-clouds;Task analysis;Three-dimensional display;Transformer;},
URL = {http://dx.doi.org/10.1109/TPAMI.2023.3259038},
}

@inproceedings{20250817900427 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2025 Elsevier Inc.},
copyright = {Compendex},
title = {PointVector: A Vector Representation in Point Cloud Analysis},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
author = {Deng, Xin and Zhang, Wenyu and Ding, Qing and Zhang, Xinming},
volume = {2023-June},
year = {2023},
pages = {9455 - 9465},
issn = {10636919},
address = {Vancouver, BC, Canada},
abstract = {<div data-language="eng" data-ev-field="abstract">In point cloud analysis, point-based methods have rapidly developed in recent years. These methods have recently focused on concise MLP structures, such as Point-NeXt, which have demonstrated competitiveness with Convolutional and Transformer structures. However, standard MLPs are limited in their ability to extract local features effectively. To address this limitation, we propose a Vector-oriented Point Set Abstraction that can aggregate neighboring features through higher-dimensional vectors. To facilitate network optimization, we construct a transformation from scalar to vector using independent angles based on 3D vector rotations. Finally, we develop a PointVector model that follows the structure of PointNeXt. Our experimental results demonstrate that PointVector achieves state-of-the-art performance 72.3% mIOU on the S3DIS Area 5 and 78.4% mIOU on the S3DIS (6-fold cross-validation) with only 58% model parameters of PointNeXt. We hope our work will help the exploration of concise and effective feature representations. The code will be released soon.<br/></div> © 2023 IEEE.},
key = {Vectors},
keywords = {3D modeling;Cloud analytics;},
note = {3d from multi-view and sensor;Cloud analysis;Local feature;Multi sensor;Multi-views;Point set;Point-based methods;Point-clouds;Transformer structure;Vector representations;},
URL = {http://dx.doi.org/10.1109/CVPR52729.2023.00912},
}




@inproceedings{20221511951298 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2025 Elsevier Inc.},
copyright = {Compendex},
title = {Walk in the Cloud: Learning Curves for Point Clouds Shape Analysis},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
author = {Xiang, Tiange and Zhang, Chaoyi and Song, Yang and Yu, Jianhui and Cai, Weidong},
year = {2021},
pages = {895 - 904},
issn = {15505499},
address = {Virtual, Online, Canada},
abstract = {<div data-language="eng" data-ev-field="abstract">Discrete point cloud objects lack sufficient shape descriptors of 3D geometries. In this paper, we present a novel method for aggregating hypothetical curves in point clouds. Sequences of connected points (curves) are initially grouped by taking guided walks in the point clouds, and then subsequently aggregated back to augment their point-wise features. We provide an effective implementation of the proposed aggregation strategy including a novel curve grouping operator followed by a curve aggregation operator. Our method was benchmarked on several point cloud analysis tasks where we achieved the state-of-the-art classification accuracy of 94.2% on the ModelNet40 classification task, instance IoU of 86.8% on the ShapeNetPart segmentation task and cosine error of 0.11 on the ModelNet40 normal estimation task. Our project page with source code is available at: https://curvenet.github.io/.<br/></div> © 2021 IEEE},
key = {Mathematical operators},
keywords = {Computer vision;},
URL = {http://dx.doi.org/10.1109/ICCV48922.2021.00095},
}

@article{20233214498628 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2025 Elsevier Inc.},
copyright = {Compendex},
title = {OctFormer: Octree-based Transformers for 3D Point Clouds},
journal = {ACM Transactions on Graphics},
author = {Wang, Peng-Shuai},
volume = {42},
number = {4},
year = {2023},
issn = {07300301},
abstract = {<div data-language="eng" data-ev-field="abstract">We propose octree-based transformers, named OctFormer, for 3D point cloud learning. OctFormer can not only serve as a general and effective backbone for 3D point cloud segmentation and object detection but also have linear complexity and is scalable for large-scale point clouds. The key challenge in applying transformers to point clouds is reducing the quadratic, thus overwhelming, computation complexity of attentions. To combat this issue, several works divide point clouds into non-overlapping windows and constrain attentions in each local window. However, the point number in each window varies greatly, impeding the efficient execution on GPU. Observing that attentions are robust to the shapes of local windows, we propose a novel octree attention, which leverages sorted shuffled keys of octrees to partition point clouds into local windows containing a fixed number of points while permitting shapes of windows to change freely. And we also introduce dilated octree attention to expand the receptive field further. Our octree attention can be implemented in 10 lines of code with open-sourced libraries and runs 17 times faster than other point cloud attentions when the point number exceeds 200k. Built upon the octree attention, OctFormer can be easily scaled up and achieves state-of-the-art performances on a series of 3D semantic segmentation and 3D object detection benchmarks, surpassing previous sparse-voxel-based CNNs and point cloud transformers in terms of both efficiency and effectiveness. Notably, on the challenging ScanNet200 dataset, OctFormer outperforms sparse-voxel-based CNNs by 7.3 in mIoU. Our code and trained models are available at https://wang-ps.github.io/octformer.<br/></div> © 2023 ACM.},
key = {Object detection},
keywords = {Object recognition;Semantic Segmentation;Semantics;},
note = {3D object;3d object detection;3D point cloud;3d semantic segmentation;Objects detection;Octrees;Point numbers;Point-clouds;Semantic segmentation;Transformer;},
URL = {http://dx.doi.org/10.1145/3592131},
}



@unpublished{20220033677 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2025 Elsevier Inc.},
copyright = {Compendex},
title = {RETHINKING NETWORK DESIGN AND LOCAL GEOMETRY IN POINT CLOUD: A SIMPLE RESIDUAL MLP FRAMEWORK},
journal = {arXiv},
author = {Ma, Xu and Qin, Can and You, Haoxuan and Ran, Haoxi and Fu, Yun},
year = {2022},
issn = {23318422},
abstract = {<div data-language="eng" data-ev-field="abstract">Point cloud analysis is challenging due to irregularity and unordered data structure. To capture the 3D geometries, prior works mainly rely on exploring sophisticated local geometric extractors using convolution, graph, or attention mechanisms. These methods, however, incur unfavorable latency during inference, and the performance saturates over the past few years. In this paper, we present a novel perspective on this task. We notice that detailed local geometrical information probably is not the key to point cloud analysis - we introduce a pure residual MLP network, called PointMLP, which integrates no "sophisticated" local geometrical extractors but still performs very competitively. Equipped with a proposed lightweight geometric affine module, PointMLP delivers the new state-of-the-art on multiple datasets. On the real-world ScanObjectNN dataset, our method even surpasses the prior best method by 3.3% accuracy. We emphasize that PointMLP achieves this strong performance without any sophisticated operations, hence leading to a superior inference speed. Compared to most recent CurveNet, PointMLP trains 2× faster, tests 7× faster, and is more accurate on ModelNet40 benchmark. We hope our PointMLP may help the community towards a better understanding of point cloud analysis. The code is available at https://github.com/ma-xu/pointMLP-pytorch.<br/></div> © 2022, CC BY-NC-ND.},
key = {Geometry},
note = {3D geometry;Attention mechanisms;Cloud analysis;Design geometry;Geometrical informations;Local geometry;Network design;Performance;Point-clouds;Simple++;},
URL = {http://dx.doi.org/10.48550/arXiv.2202.07123},
}


@article{20215211398969 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2025 Elsevier Inc.},
copyright = {Compendex},
title = {Rethinking 3-D LiDAR Point Cloud Segmentation},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
author = {Li, Shijie and Liu, Yun and Gall, Juergen},
volume = {36},
number = {3},
year = {2025},
pages = {4079 - 4090},
issn = {2162237X},
abstract = {<div data-language="eng" data-ev-field="abstract">Many point-based semantic segmentation methods have been designed for indoor scenarios, but they struggle if they are applied to point clouds that are captured by a light detection and ranging (LiDAR) sensor in an outdoor environment. In order to make these methods more efficient and robust such that they can handle LiDAR data, we introduce the general concept of reformulating 3-D point-based operations such that they can operate in the projection space. While we show by means of three point-based methods that the reformulated versions are between 300 and 400 times faster and achieve higher accuracy, we furthermore demonstrate that the concept of reformulating 3-D point-based operations allows to design new architectures that unify the benefits of point-based and image-based methods. As an example, we introduce a network that integrates reformulated 3-D point-based operations into a 2-D encoder-decoder architecture that fuses the information from different 2-D scales. We evaluate the approach on four challenging datasets for semantic LiDAR point cloud segmentation and show that leveraging reformulated 3-D point-based operations with 2-D image-based operations achieves very good results for all four datasets.<br/></div> © 2021 IEEE.},
key = {Semantics},
keywords = {Autonomous vehicles;Computer architecture;Optical radar;Semantic Segmentation;Tracking radar;},
note = {Autonomous driving;Autonomous Vehicles;Detection sensors;Images segmentations;Light detection and ranging;Light detection and ranging sensor;Point cloud compression;Point-clouds;Ranging sensors;Semantic segmentation;Semantic segmentation.;Task analysis;},
URL = {http://dx.doi.org/10.1109/TNNLS.2021.3132836},
}

@article{20240915652025 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2025 Elsevier Inc.},
copyright = {Compendex},
title = {PointWavelet: Learning in Spectral Domain for 3-D Point Cloud Analysis},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
author = {Wen, Cheng and Long, Jianzhi and Yu, Baosheng and Tao, Dacheng},
volume = {36},
number = {3},
year = {2025},
pages = {4400 - 4412},
issn = {2162237X},
abstract = {<div data-language="eng" data-ev-field="abstract">With recent success of deep learning in 2-D visual recognition, deep-learning-based 3-D point cloud analysis has received increasing attention from the community, especially due to the rapid development of autonomous driving technologies. However, most existing methods directly learn point features in the spatial domain, leaving the local structures in the spectral domain poorly investigated. In this article, we introduce a new method, PointWavelet, to explore local graphs in the spectral domain via a learnable graph wavelet transform. Specifically, we first introduce the graph wavelet transform to form multiscale spectral graph convolution to learn effective local structural representations. To avoid the time-consuming spectral decomposition, we then devise a learnable graph wavelet transform, which significantly accelerates the overall training process. Extensive experiments on four popular point cloud datasets, ModelNet40, ScanObjectNN, ShapeNet-Part, and S3DIS, demonstrate the effectiveness of the proposed method on point cloud classification and segmentation.<br/></div> © 2024 IEEE.},
key = {Spectrum analysis},
keywords = {Classification (of information);Deep learning;Fourier transforms;Image analysis;Laplace transforms;Strain measurement;Wavelet decomposition;},
note = {Features extraction;Graph wavelets;Learnable wavelet transform;Point cloud compression;Point-clouds;Spectral domains;Wavelet domain;Wavelets transform;},
URL = {http://dx.doi.org/10.1109/TNNLS.2024.3363244},
}

@article{20221211818723 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2025 Elsevier Inc.},
copyright = {Compendex},
title = {A Novel Local-Global Graph Convolutional Method for Point Cloud Semantic Segmentation},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
author = {Du, Zijin and Ye, Hailiang and Cao, Feilong},
volume = {35},
number = {4},
year = {2024},
pages = {4798 - 4812},
issn = {2162237X},
abstract = {<div data-language="eng" data-ev-field="abstract">Although convolutional neural networks (CNNs) have shown good performance on grid data, they are limited in the semantic segmentation of irregular point clouds. This article proposes a novel and effective graph CNN framework, referred to as the local-global graph convolutional method (LGGCM), which can achieve short- and long-range dependencies on point clouds. The key to this framework is the design of local spatial attention convolution (LSA-Conv). The design includes two parts: generating a weighted adjacency matrix of the local graph composed of neighborhood points, and updating and aggregating the features of nodes to obtain the spatial geometric features of the local point cloud. In addition, a smooth module for central points is incorporated into the process of LSA-Conv to enhance the robustness of the convolution against noise interference by adjusting the position coordinates of the points adaptively. The learned robust LSA-Conv features are then fed into a global spatial attention module with the gated unit to extract long-range contextual information and dynamically adjust the weights of features from different stages. The proposed framework, consisting of both encoding and decoding branches, is an end-to-end trainable network for semantic segmentation of 3-D point clouds. The theoretical analysis of the approximation capabilities of LSA-Conv is discussed to determine whether the features of the point cloud can be accurately represented. Experimental results on challenging benchmarks of the 3-D point cloud demonstrate that the proposed framework achieves excellent performance.<br/></div> © 2012 IEEE.},
key = {Semantics},
keywords = {Benchmarking;Convolution;Deep learning;Neural networks;Semantic Segmentation;Semantic Web;},
note = {Attention mechanisms;Convolutional neural network;Correlation;Deep learning;Features extraction;Graph convolutional neural network;Point cloud compression;Point-clouds;Semantic segmentation;Semantic segmentation.;},
URL = {http://dx.doi.org/10.1109/TNNLS.2022.3155282},
}

@article{20221511959717 ,
language = {English},
copyright = {Compilation and indexing terms, Copyright 2025 Elsevier Inc.},
copyright = {Compendex},
title = {Dual-Graph Attention Convolution Network for 3-D Point Cloud Classification},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
author = {Huang, Chang-Qin and Jiang, Fan and Huang, Qiong-Hao and Wang, Xi-Zhe and Han, Zhong-Mei and Huang, Wei-Yu},
volume = {35},
number = {4},
year = {2024},
pages = {4813 - 4825},
issn = {2162237X},
abstract = {<div data-language="eng" data-ev-field="abstract">Three-dimensional point cloud classification is fundamental but still challenging in 3-D vision. Existing graph-based deep learning methods fail to learn both low-level extrinsic and high-level intrinsic features together. These two levels of features are critical to improving classification accuracy. To this end, we propose a dual-graph attention convolution network (DGACN). The idea of DGACN is to use two types of graph attention convolution operations with a feedback graph feature fusion mechanism. Specifically, we exploit graph geometric attention convolution to capture low-level extrinsic features in 3-D space. Furthermore, we apply graph embedding attention convolution to learn multiscale low-level extrinsic and high-level intrinsic fused graph features together. Moreover, the points belonging to different parts in real-world 3-D point cloud objects are distinguished, which results in more robust performance for 3-D point cloud classification tasks than other competitive methods, in practice. Our extensive experimental results show that the proposed network achieves state-of-the-art performance on both the synthetic ModelNet40 and real-world ScanObjectNN datasets.<br/></div> © 2012 IEEE.},
key = {Convolution},
keywords = {Computer vision;Deep learning;Graphic methods;Neural networks;},
note = {3-D point cloud;Attention mechanisms;Convolutional neural network;Deep learning;Features extraction;Geometric attention mechanism;Graph convolution network;Intrinsic and extrinsic feature.;Point cloud compression;Point-clouds;Shape;},
URL = {http://dx.doi.org/10.1109/TNNLS.2022.3162301},
}




